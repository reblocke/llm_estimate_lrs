{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Scraper and Estimator\n",
    "\n",
    "Overall, this notebook contains code to scrape diagnostic likelihood ratios from theNNT.com and convert them to numerical form. \n",
    "\n",
    "It also contains code to generate prompts for large language models to estimate the likelihood ratios. \n",
    "\n",
    "The output is a spreadsheet called: nnt_lrs_with_estimated which contains: \n",
    "- a sheet for each diagnosis or prediction target\n",
    "- a row for each piece of information\n",
    "- columns for the name, raw nnt lr, processed nnt lr, and estimated by 1 or more LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data from the NNT \n",
    "\n",
    "This scrapes all of the likelihood ratios listed on the NNT ('https://thennt.com/home-lr/') into Excel spreadsheets. \n",
    "\n",
    "1. A spreadsheet (\"nnt_lrs.xlsx\") contains a separate sheet for each page, which corresponds to a \"prediction tasks\" e.g. diagnosing the cause of a symptom - sometimes with specification of an intended population. Each sheet contains two columns: the name of the features (e.g. test result, finding, historical occurence, comorbditiy), the second contains the raw listing from the spreadsheet\n",
    "\n",
    "2. A second spreadsheet contains the same sheets corresponding to a prediction target, and all of the features. These are used two call the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI  # or your appropriate client wrapper\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # looks for a .env file in the current dir by default\n",
    "#print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specialty_links():\n",
    "    \"\"\"\n",
    "    Extracts specialties and their corresponding article links from the webpage.\n",
    "    Returns a list of dictionaries with specialty names and associated links.\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://thennt.com/home-lr/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the section with \"Diagnosis (LR) Reviews by Specialty\"\n",
    "    specialty_section = soup.find('div', class_='well subdisplay accordion_caption', id='lr-byspecialty')\n",
    "\n",
    "    if not specialty_section:\n",
    "        print(\"Could not find the 'Diagnosis (LR) Reviews by Specialty' section on the webpage.\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Find all specialty headings (e.g., h3)\n",
    "    subheadings = specialty_section.find_all('h3')\n",
    "\n",
    "    for subheading in subheadings:\n",
    "        subheading_text = subheading.get_text(strip=True)  # Get specialty name\n",
    "        links = []\n",
    "\n",
    "        # Find the next unordered list (ul) which contains links\n",
    "        next_ul = subheading.find_next_sibling('ul')\n",
    "\n",
    "        if next_ul:\n",
    "            for a_tag in next_ul.find_all('a', href=True):\n",
    "                link_text = a_tag.get_text(strip=True)  # Link display name\n",
    "                link_href = a_tag['href']  # Actual URL\n",
    "                links.append({'display_name': link_text, 'url': link_href})\n",
    "\n",
    "        results.append({'specialty': subheading_text, 'links': links})\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_likelihood_ratios(page_content):\n",
    "    \"\"\"\n",
    "    Parses all likelihood ratio tables within <article class=\"lr_cards_details\">.\n",
    "    For each subsection indicated by an <h3> heading:\n",
    "      - If the heading indicates Positive Findings, each finding will be prefixed with \"Patient has: \".\n",
    "      - If the heading indicates Negative Findings, a leading \"No\" (if present) is removed from the finding and then it is prefixed with \"Patient does not have: \".\n",
    "    This function processes all tables under a given heading (i.e. until the next <h3> is reached).\n",
    "    Returns a list of tuples: (finding, likelihood ratio).\n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    results = []\n",
    "    \n",
    "    # Locate the main LR details section.\n",
    "    lr_section = soup.find('article', class_='lr_cards_details')\n",
    "    if not lr_section:\n",
    "        return results\n",
    "\n",
    "    # Find all <h3> headings in the section.\n",
    "    headings = lr_section.find_all('h3')\n",
    "    \n",
    "    if headings:\n",
    "        for h3 in headings:\n",
    "            heading_text = h3.get_text(strip=True)\n",
    "            if \"Positive Findings\" in heading_text:\n",
    "                prefix = \"Patient has: \"\n",
    "            elif \"Negative Findings\" in heading_text:\n",
    "                prefix = \"Patient does not have: \"\n",
    "            else:\n",
    "                prefix = \"\"\n",
    "            \n",
    "            # Process all sibling elements until the next <h3> is encountered.\n",
    "            sibling = h3.find_next_sibling()\n",
    "            while sibling and sibling.name != \"h3\":\n",
    "                if sibling.name == \"table\" and \"lrtable\" in sibling.get(\"class\", []):\n",
    "                    # Try to get proper data rows (i.e. <tr> elements with <td>).\n",
    "                    rows = sibling.find_all(\"tr\")\n",
    "                    data_rows = [row for row in rows if row.find_all(\"td\")]\n",
    "                    \n",
    "                    if data_rows:\n",
    "                        for row in data_rows:\n",
    "                            cols = row.find_all(\"td\")\n",
    "                            if len(cols) >= 2:\n",
    "                                finding = cols[0].get_text(strip=True)\n",
    "                                lr_value = cols[1].get_text(strip=True)\n",
    "                                # If there's an <a> inside the LR cell, use its text.\n",
    "                                link = cols[1].find(\"a\")\n",
    "                                if link:\n",
    "                                    lr_value = link.get_text(strip=True) or lr_value\n",
    "                                if not lr_value:\n",
    "                                    lr_value = \"Not reported\"\n",
    "                                \n",
    "                                # Modify the finding string based on the prefix.\n",
    "                                if prefix:\n",
    "                                    if prefix.startswith(\"Patient does not have:\"):\n",
    "                                        finding = re.sub(r'^no\\s+', '', finding, flags=re.IGNORECASE)\n",
    "                                    finding = prefix + finding\n",
    "                                \n",
    "                                results.append((finding, lr_value))\n",
    "                    else:\n",
    "                        # If no rows with <td> are found, assume the table contains <td> elements in sequence.\n",
    "                        all_tds = sibling.find_all(\"td\")\n",
    "                        # Process in pairs.\n",
    "                        for i in range(0, len(all_tds), 2):\n",
    "                            finding = all_tds[i].get_text(strip=True)\n",
    "                            if i+1 < len(all_tds):\n",
    "                                lr_value = all_tds[i+1].get_text(strip=True)\n",
    "                            else:\n",
    "                                lr_value = \"Not reported\"\n",
    "                            # Check for an <a> element.\n",
    "                            a_tag = all_tds[i+1].find(\"a\")\n",
    "                            if a_tag:\n",
    "                                lr_value = a_tag.get_text(strip=True) or lr_value\n",
    "                            if not lr_value:\n",
    "                                lr_value = \"Not reported\"\n",
    "                            \n",
    "                            if prefix:\n",
    "                                if prefix.startswith(\"Patient does not have:\"):\n",
    "                                    finding = re.sub(r'^no\\s+', '', finding, flags=re.IGNORECASE)\n",
    "                                finding = prefix + finding\n",
    "                            results.append((finding, lr_value))\n",
    "                sibling = sibling.find_next_sibling()\n",
    "    else:\n",
    "        # Fallback: process all tables in the section if no <h3> headings exist.\n",
    "        tables = lr_section.find_all('table', class_='lrtable')\n",
    "        for table in tables:\n",
    "            rows = table.find_all(\"tr\")\n",
    "            data_rows = [row for row in rows if row.find_all(\"td\")]\n",
    "            if data_rows:\n",
    "                for row in data_rows:\n",
    "                    cols = row.find_all(\"td\")\n",
    "                    if len(cols) >= 2:\n",
    "                        finding = cols[0].get_text(strip=True)\n",
    "                        lr_value = cols[1].get_text(strip=True)\n",
    "                        link = cols[1].find(\"a\")\n",
    "                        if link:\n",
    "                            lr_value = link.get_text(strip=True) or lr_value\n",
    "                        if not lr_value:\n",
    "                            lr_value = \"Not reported\"\n",
    "                        results.append((finding, lr_value))\n",
    "            else:\n",
    "                all_tds = table.find_all(\"td\")\n",
    "                for i in range(0, len(all_tds), 2):\n",
    "                    finding = all_tds[i].get_text(strip=True)\n",
    "                    if i+1 < len(all_tds):\n",
    "                        lr_value = all_tds[i+1].get_text(strip=True)\n",
    "                    else:\n",
    "                        lr_value = \"Not reported\"\n",
    "                    a_tag = all_tds[i+1].find(\"a\") if i+1 < len(all_tds) else None\n",
    "                    if a_tag:\n",
    "                        lr_value = a_tag.get_text(strip=True) or lr_value\n",
    "                    if not lr_value:\n",
    "                        lr_value = \"Not reported\"\n",
    "                    results.append((finding, lr_value))\n",
    "                    \n",
    "    return results\n",
    "\n",
    "\"\"\"\n",
    "def extract_likelihood_ratios(page_content):\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    results = []\n",
    "\n",
    "    # Locate the section containing likelihood ratio tables\n",
    "    lr_section = soup.find('article', class_='lr_cards_details')\n",
    "    if not lr_section:\n",
    "        return results  # Return empty if no section found\n",
    "\n",
    "    # Find all tables inside the LR card\n",
    "    tables = lr_section.find_all('table', class_='lrtable')\n",
    "\n",
    "    for table in tables:\n",
    "        # Grab all <tr> elements\n",
    "        all_rows = table.find_all('tr')\n",
    "\n",
    "        # Filter out any row that only has <th> (i.e., a header row)\n",
    "        data_rows = []\n",
    "        for row in all_rows:\n",
    "            # If there's at least one <td> in this row, treat it as a data row\n",
    "            if row.find_all('td'):\n",
    "                data_rows.append(row)\n",
    "\n",
    "        # If we have real data rows, parse them\n",
    "        if data_rows:\n",
    "            for row in data_rows:\n",
    "                cols = row.find_all('td')\n",
    "                # If the row has exactly 2 <td>, treat them as (finding, LR)\n",
    "                if len(cols) == 2:\n",
    "                    finding = cols[0].get_text(strip=True)\n",
    "                    lr_value = cols[1].get_text(strip=True)\n",
    "                    # If there's an <a> inside the LR cell, grab its text\n",
    "                    link = cols[1].find('a')\n",
    "                    if link:\n",
    "                        lr_value = link.get_text(strip=True) or lr_value\n",
    "                    if not lr_value:\n",
    "                        lr_value = \"Not reported\"\n",
    "\n",
    "                    results.append((finding, lr_value))\n",
    "\n",
    "        else:\n",
    "            # Fallback: if there are no valid data rows, we process all <td> in pairs\n",
    "            cols = table.find_all('td')\n",
    "            for i in range(0, len(cols) - 1, 2):\n",
    "                finding = cols[i].get_text(strip=True)\n",
    "                lr_value_element = cols[i + 1]\n",
    "\n",
    "                # Extract the likelihood ratio, handling nested <a> and <br/>\n",
    "                link = lr_value_element.find('a')\n",
    "                if link:\n",
    "                    lr_value = link.get_text(strip=True)\n",
    "                else:\n",
    "                    lr_value = lr_value_element.get_text(strip=True)\n",
    "\n",
    "                if not lr_value:\n",
    "                    lr_value = \"Not reported\"\n",
    "\n",
    "                results.append((finding, lr_value))\n",
    "\n",
    "    return results\n",
    "\"\"\"\n",
    "    \n",
    "def fetch_webpages(specialty_links):\n",
    "    \"\"\"\n",
    "    Iterates through all the extracted links, fetches the webpage content, \n",
    "    and extracts likelihood ratio findings.\n",
    "    \"\"\"\n",
    "    findings_by_display_name = {}\n",
    "\n",
    "    for item in specialty_links:\n",
    "        print(f\"Fetching pages for Specialty: {item['specialty']}\")\n",
    "\n",
    "        for link in item['links']:\n",
    "            display_name = link['display_name']\n",
    "            url = link['url']\n",
    "\n",
    "            try:\n",
    "                print(f\"  - Fetching: {display_name} ({url})\")\n",
    "                response = requests.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"    Success: {display_name} page fetched.\")\n",
    "                    \n",
    "                    # Extract likelihood ratio findings\n",
    "                    findings = extract_likelihood_ratios(response.text)\n",
    "                    \n",
    "                    # Store the extracted data\n",
    "                    findings_by_display_name[display_name] = findings\n",
    "\n",
    "                else:\n",
    "                    print(f\"    Failed to fetch {display_name} - Status Code: {response.status_code}\")\n",
    "\n",
    "                time.sleep(1)  # Optional: Add a delay to avoid overwhelming the server\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"    Error fetching {display_name}: {e}\")\n",
    "\n",
    "        print(\"\\n\")  # Add space between specialties for readability\n",
    "\n",
    "    return findings_by_display_name\n",
    "\n",
    "def save_to_excel(findings_data, filename=\"nnt_lrs.xlsx\", blank_values=False):\n",
    "    \"\"\"\n",
    "    Saves likelihood ratios to an Excel file with each display_name as a separate sheet.\n",
    "    If blank_values is True, the Likelihood Ratio column is left blank.\n",
    "    The first row contains the full display_name, and column headers start from the second row.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
    "        for display_name, findings in findings_data.items():\n",
    "            if findings:\n",
    "                # Prepare DataFrame\n",
    "                df = pd.DataFrame(findings, columns=[\"Finding\", \"Likelihood Ratio\"])\n",
    "\n",
    "                if blank_values:\n",
    "                    df[\"Likelihood Ratio\"] = \"\"  # Clear likelihood ratio values\n",
    "\n",
    "                # Insert full display_name as the first row\n",
    "                full_name_row = pd.DataFrame({df.columns[0]: [display_name], df.columns[1]: [\"\"]})\n",
    "                df = pd.concat([full_name_row, df], ignore_index=True)\n",
    "\n",
    "                # Save to Excel with sheet name as the **last** 31 characters\n",
    "                sheet_name = display_name[-31:]\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)  # No default header\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping {display_name} (No data found).\")\n",
    "\n",
    "    print(f\"\\nLikelihood ratios saved to {filename}\")\n",
    "\n",
    "# Fetch specialties and links\n",
    "specialty_links = get_specialty_links()\n",
    "findings_data = fetch_webpages(specialty_links)\n",
    "\n",
    "# Save normal file\n",
    "save_to_excel(findings_data, \"nnt_lrs.xlsx\", blank_values=False)\n",
    "\n",
    "# Save version with blank likelihood ratios\n",
    "save_to_excel(findings_data, \"nnt_lrs_sans_number.xlsx\", blank_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert LR's to numerical format\n",
    "\n",
    "This block takes in the excel spreadsheet with raw data from theNNT.com (\"nnt_lrs.xlsx\") and creates a new spreadsheet (\"nnt_lrs_processed.xlsx\") with a third column that contains a numerical version of the second column (raw data from theNNT) to be used as the LR_llm.\n",
    "\n",
    "\n",
    "It removes any 'x's from the input, then determines whether the cell reports \n",
    "\n",
    "1. point estimate only (in which case use the point estimate)\n",
    "2. point estimate + range (in which case take the point estimate), or \n",
    "3. range only (in which case, calculate the geometric mean)\n",
    "\n",
    "It also counts the number of conditions (last 31 letters due to excel limitation) and LRs\n",
    "\n",
    "\n",
    "[ ] TODO: \n",
    "- some of the BNP thresholds just have a number rather than a specification of the full \"BNP > 100\"; need to use header e.g. https://thennt.com/lr/dyspnea-due-to-heart-failure-without-chronic-respiratory-disease/ and https://thennt.com/lr/dyspnea-due-to-heart-failure-without-chronic-respiratory-disease/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lr(lr_str):\n",
    "    \"\"\"\n",
    "    Given a string from the 'Likelihood Ratio' cell, this function:\n",
    "      - Removes any 'x' characters from the input.\n",
    "      - If the string contains a parenthesized range (i.e. a point estimate plus a range),\n",
    "        it returns the point estimate.\n",
    "      - If the entire string is a range (e.g. \"0.92-1.1\", \"3.3 to 4.8\", \"4.8–7.6\"),\n",
    "        it computes and returns the geometric mean.\n",
    "      - Otherwise, it returns a float based on the first number found.\n",
    "      - If the value is missing or cannot be parsed, returns NaN.\n",
    "    \"\"\"\n",
    "    # Remove all 'x' characters and trim whitespace\n",
    "    lr_str = lr_str.replace(\"x\", \"\").strip()\n",
    "    if lr_str == \"\":\n",
    "        return np.nan\n",
    "\n",
    "    # If parentheses exist, assume format \"point_estimate (range)\" and use the point estimate.\n",
    "    if \"(\" in lr_str:\n",
    "        point_part = lr_str.split(\"(\")[0].strip()\n",
    "        try:\n",
    "            return float(point_part)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Check for a range-only pattern.\n",
    "    # This regex looks for two numbers separated by \"to\", \"-\" or \"–\" with optional whitespace.\n",
    "    range_only_match = re.match(r'^\\s*([0-9]*\\.?[0-9]+)\\s*(to|[-–])\\s*([0-9]*\\.?[0-9]+)\\s*$', lr_str)\n",
    "    if range_only_match:\n",
    "        try:\n",
    "            low = float(range_only_match.group(1))\n",
    "            high = float(range_only_match.group(3))\n",
    "            return math.sqrt(low * high)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    # Fallback: if no range-only pattern is found, extract the first number and return it.\n",
    "    numbers = re.findall(r'([0-9]*\\.?[0-9]+)', lr_str)\n",
    "    if numbers:\n",
    "        try:\n",
    "            return float(numbers[0])\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Load the original Excel file (each sheet has no header and the first row is the display name row)\n",
    "input_filename = \"nnt_lrs.xlsx\"\n",
    "output_filename = \"nnt_lrs_processed.xlsx\"\n",
    "\n",
    "# Read all sheets from the Excel file into a dictionary of DataFrames.\n",
    "excel_sheets = pd.read_excel(input_filename, sheet_name=None, header=None)\n",
    "\n",
    "total_lr_count = 0\n",
    "sheet_counts = {}\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine=\"openpyxl\") as writer:\n",
    "    for sheet_name, df in excel_sheets.items():\n",
    "        numerical_lr = []\n",
    "        # Process each row in the sheet.\n",
    "        for idx, row in df.iterrows():\n",
    "            # For the header row (assumed to be the first row: condition label), add an empty string.\n",
    "            if idx == 0:\n",
    "                numerical_lr.append(\"\")\n",
    "            else:\n",
    "                cell_value = row[1]  # The original \"Likelihood Ratio\" is in the second column (index 1)\n",
    "                if pd.isna(cell_value) or str(cell_value).strip() == \"\":\n",
    "                    numerical_lr.append(\"\")\n",
    "                else:\n",
    "                    numerical_lr.append(parse_lr(str(cell_value)))\n",
    "        \n",
    "        # Insert the new column immediately after the \"Likelihood Ratio\" column.\n",
    "        # This makes the new column the third column.\n",
    "        df.insert(2, \"Numerical LR\", numerical_lr)\n",
    "        \n",
    "        # Remove rows (except the header) where the new \"Numerical LR\" is empty or NaN.\n",
    "        header = df.iloc[[0]]  # Keep the header row (the condition label)\n",
    "        data = df.iloc[1:]\n",
    "        data = data[data[\"Numerical LR\"].apply(lambda x: not (x == \"\" or pd.isna(x)))]\n",
    "        filtered_df = pd.concat([header, data], ignore_index=True)\n",
    "        \n",
    "        # Insert a new row (after the condition label row) with the column labels.\n",
    "        # The final sheet will have:\n",
    "        #   Row 0: Condition label (from the original sheet)\n",
    "        #   Row 1: Column labels: 'finding', 'lr_raw', and 'lr_num'\n",
    "        #   Row 2+: Data rows\n",
    "        col_labels = pd.DataFrame([[\"finding\", \"lr_raw\", \"lr_reported\"]], columns=filtered_df.columns)\n",
    "        final_df = pd.concat([filtered_df.iloc[[0]], col_labels, filtered_df.iloc[1:]], ignore_index=True)\n",
    "        \n",
    "        # Count the number of LR values for this sheet (exclude the two header rows).\n",
    "        lr_count = len(final_df) - 2\n",
    "        sheet_counts[sheet_name] = lr_count\n",
    "        total_lr_count += lr_count\n",
    "        \n",
    "        # Write the modified DataFrame to the new Excel file.\n",
    "        # The output maintains the original format: no index and no additional header row.\n",
    "        final_df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "# Display counts.\n",
    "num_sheets = len(excel_sheets)\n",
    "print(f\"Processed {num_sheets} condition(s) (sheets).\")\n",
    "for sheet, count in sheet_counts.items():\n",
    "    print(f\"Sheet '{sheet}' has {count} LR value(s).\")\n",
    "print(f\"Total LR values processed across all sheets: {total_lr_count}.\")\n",
    "\n",
    "print(f\"Processed Excel file saved as '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate LRs\n",
    "\n",
    "NOTE: for the real run at this, we'll want to do some manual editing of the info columns - as there are some where it is a lab value that references the preceeding value (not currently automated to account for). \n",
    "--- particularly, BNP thresholds in the cardiac-cause sheets. \n",
    "\n",
    "\n",
    "This code block reads in the data from the nnt_lr_processed.xlsx excel file and calls a list of openAI models to have them give there best (single) estimate of the LR. Then, it rights a new spreadsheet nnt_lr_estimates that includes columns in each spreadsheet for each estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the response schema expecting a floating point number.\n",
    "class LRResponse(BaseModel):\n",
    "    value: float\n",
    "\n",
    "def estimate_lr(diagnosis: str, info_val: str, client, model: str) -> float:\n",
    "    \"\"\"\n",
    "    Calls the LLM with a prompt containing the diagnosis and a finding.\n",
    "    Returns the estimated likelihood ratio as a floating point number.\n",
    "    \"\"\"\n",
    "    lr_prompt = \"\"\"You are an expert in medical diagnosis who is giving assessments of how important a piece of information is when determining whether a patient has a particularly condition. Your task is to estimate the likelihood ratio of a finding for a disease. Recall that the likelihood ratio represents how much the ratio between the odds of disease given a result for a lab value, whether a physical exam finding is present, or whether a comorbidity is present over the odds of disease when you did not know the result.\n",
    "You will receive inputs in the following format; Target condition: <Condition, e.g. Patient Has: Cardiac chest pain>. Finding: <piece of information, e.g. ‘Patient does not have: radiation to the neck, arm, or jaw’>.\n",
    "So, for example. If the odds of a Condition Z being present was 1 (meaning 50% probability) before we knew anything, but then we got a result (Finding A) it became 2 (meaning 2:1 odds or 66% probability), then the likelihood ratio would be 2. \n",
    "Given a condition and a finding, you will provide your best estimate of the likelihood ratio as a floating point number. Return your answer in valid JSON with the following schema: { 'value': <floating point number greater than 0> }.\\n\\n\n",
    "\n",
    "Remember, stronger evidence in favor of a condition has a value farther above 1. Strong evidence against a diagnosis has a value farther below 1 (closer to 0). A likelihood ratio of 10 is equally strong evidence for a condition as a likelihood ratio of 0.1 is against it. Likelihood ratios near 1 represent weak evidence for or against. \n",
    "And if the \"patient does not have: \" some feature that is almost always present, that is strong evidence against.\n",
    "(pay attention for double negatives- Patient has: no tobacco and Patient does not have: tobacco are identical)\n",
    "\n",
    "Here is how I would like you to approach the problem:\n",
    "First, consider the condition you are predicting (Condition: ___). Is the condition a medical diagnosis? If so, what kind of findings are usually present in someone who has that condition. Does the condition specify a certain type of patient? If so, how does that change things? \n",
    "Then, consider the finding. If a finding is much more common among patients who have the condition of interest than among patients who do not have the condition of interest, then the likelihood ratio should be high. This might be because the finding is a consequence of the disease, indicates that an enabling condition is present, indicates that a frequently comorbid condition is present, or is related to the pathology of the condition. In general, likelihood ratios over about 20 are pathognomonic, above 5 or so is extremely strong evidence in favor, above 2.5 or so is strong evidence, above 1.4 is so-so evidence, and 1-1.4 is pretty weak evidence. Conversely, if the finding is more common in people who do NOT have the condition, then the likelihood ratio should be below 1. Similarly, a likelihood ratio below 0.05 would exclude the condition in most situations, below 0.2 would be extremely strong evidence against, below 0.4 would be strong evidence against, below 0.71 is so-so, and between 0.71 and 1 is pretty weak evidence against (meaning, it just doesn’t change the odds of the condition much). \n",
    "\n",
    "Here are some hypothetical examples to consider: \n",
    "    Prompt = Target condition: Cardiac Chest Pain. Finding: Patient has: Pain not worse with exertion (requires they clarify exercise 1hr after meal).\n",
    "    You would reason that because cardiac chest pain is usually worse with exertion because exertion worsens cardiac demand for oxygen, and thus worsens ischemia.\n",
    "    Response = {\n",
    "        ‘value’: 0.4\n",
    "    }\n",
    "\n",
    "    Prompt =  Target condition: Cardiac Chest Pain. Finding: Patient does not have: tobacco.\n",
    "    You would reason that because being someone who smokes increases your risk of coronary artery disease, and thus being a never smoker means you’re at less risk… but many people who have heart attacks still smoke, so it’s only a weak predictor. \n",
    "    Response = {\n",
    "        ‘value’: 0.75\n",
    "    }\n",
    "\n",
    "    Prompt = Target condition: Cardaic Chest Pain. Finding = Patient has: enjoys playing chess.\n",
    "    You would reason that because enjoying chest has no relationship to having a heart attack.\n",
    "    Response = {\n",
    "        ‘value’: 1\n",
    "    }\n",
    "\n",
    "    Prompt = Target condition: Cardiac Chest Pain. Finding = Patient has: pain located behind the sternum\n",
    "    You would reason that because cardiac chest pain is often experienced behind the sternum (thus, more likely), but so are many other causes of chest pain - like GERD.\n",
    "    Response = {\n",
    "        ‘value’: 1.2\n",
    "    }\n",
    "\n",
    "    Prompt = Condition: Cardiac Chest Pain. Finding = patient has: pain worse with exertion.\n",
    "    You would reason that because the increased myocardial oxygen consumption worsens the pain if oxygen delivery to the myocardium is the cause, as it is in heart attacks.\n",
    "    Response = {\n",
    "        ‘value’: 3.4\n",
    "    }\n",
    "\n",
    "    OK: here’s the prompt.. \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": lr_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Condition: {diagnosis}\\nFinding: {info_val}\"}\n",
    "    ]\n",
    "\n",
    "    # Check if the model starts with \"o3-mini\" \"o3\" \"o4-mini, \"o4\", etc.\n",
    "    kwargs = {}\n",
    "    if model.startswith(\"o\"):\n",
    "        kwargs[\"reasoning_effort\"] = \"medium\"  # low, medium, high depending on need\n",
    "\n",
    "    # Call the LLM using the provided model name.\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=LRResponse,\n",
    "        **kwargs  # pass the conditional keyword argument\n",
    "    )\n",
    "\n",
    "    # Call the LLM using the provided model name.\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,  # use the current model from the list\n",
    "        messages=messages,\n",
    "        response_format=LRResponse,\n",
    "    )\n",
    "    \n",
    "    # Extract and return the floating point estimate.\n",
    "    lr_response = completion.choices[0].message.parsed\n",
    "    return lr_response.value\n",
    "\n",
    "\n",
    "# Initialize the OpenAI (or your chosen) client using your API key.\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# List of model names to iterate over.\n",
    "model_names = ['gpt-4o-mini-2024-07-18', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-2025-04-14', 'o4-mini-2025-04-16'] #'gpt-4o-2024-08-06', 'o3-mini-2025-01-31', gpt-4.1-2025-04-14, o4-mini-2025-04-16, 'o3-2025-04-16']\n",
    "\n",
    "# Read the processed Excel file.\n",
    "# We use header=None so that row 0 (the diagnosis row) and row 1 (the column headers row) are preserved.\n",
    "input_filename = \"nnt_lrs_processed.xlsx\"\n",
    "#input_filename = \"new_\" \\\n",
    "#\"nnt_lrs_processed.xlsx\" # use this one for the substitute in\n",
    "sheets = pd.read_excel(input_filename, sheet_name=None, header=None)\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    # Set diagnosis from the first row (row index 0, first cell)\n",
    "    diagnosis = df.iloc[0, 0]\n",
    "\n",
    "    # For each model in the list, call the LLM and add a new column with the estimation.\n",
    "    for model in model_names:\n",
    "        new_col_header = \"lr_\" + model\n",
    "        new_col = []  # This list will hold one value per row\n",
    "        print(f\"Diagnosis: '{diagnosis}', Model: '{model}'\")\n",
    "\n",
    "        # Iterate over each row in the sheet.\n",
    "        # Row 0 is the diagnosis row; row 1 is the existing column labels.\n",
    "        for i in range(len(df)):\n",
    "            if i == 0:\n",
    "                new_col.append(\"\")  # Leave the diagnosis row unchanged.\n",
    "            elif i == 1:\n",
    "                new_col.append(new_col_header)  # Insert the new column header in row 1.\n",
    "            else:\n",
    "                # For data rows, use the \"finding\" from the first column (index 0)\n",
    "                info_val = df.iloc[i, 0]\n",
    "                try:\n",
    "                    estimated_lr = estimate_lr(diagnosis, info_val, client, model)\n",
    "                except Exception as e:\n",
    "                    estimated_lr = \"ERROR\"  \n",
    "                    print(f\"Error estimating LR for sheet '{sheet_name}', row {i}, model {model}: {e}\")\n",
    "                new_col.append(estimated_lr)\n",
    "        \n",
    "        # Insert the new column at the end of the dataframe.\n",
    "        df.insert(df.shape[1], new_col_header, new_col)\n",
    "    \n",
    "    # Update the sheet data in our dictionary.\n",
    "    sheets[sheet_name] = df\n",
    "\n",
    "# Write out the modified sheets to a new Excel file.\n",
    "output_filename = \"nnt_lrs_with_estimated.xlsx\"\n",
    "with pd.ExcelWriter(output_filename, engine=\"openpyxl\") as writer:\n",
    "    for sheet_name, df in sheets.items():\n",
    "        # Write without adding pandas default headers or indices.\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "print(f\"Processed Excel file saved as '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bayesian LR estimator – tolerant of o‑family hidden‑reasoning bloat\n",
    "2025‑07‑07\n",
    "\"\"\"\n",
    "import os, logging, pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# SYSTEM PROMPT – still enforces JSON-only output\n",
    "# --------------------------------------------------------------------------\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a Bayesian-diagnostic assistant.\\n\"\n",
    "    \"Think step-by-step internally, then output only JSON:\\n\"\n",
    "    '{\"value\": <float> (>0)}.'\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# INTERNAL (HIDDEN) REASONING CHECKLIST\n",
    "# --------------------------------------------------------------------------\n",
    "INTERNAL_CHECKLIST = (\n",
    "    \"Use this silent checklist:\\n\"\n",
    "    \" 1. Recall the typical presentation.\\n\"\n",
    "    \" 2. Consider if the findings is present or absent.\\n\"\n",
    "    \" 3. Compare frequency of the finding (or its absence) in positives vs negatives.\\n\"\n",
    "    \" 4. Use the LR guide as a rough reference.\\n\"\n",
    "    \" 5. Refine your estimate within the LR-guide categories (e.g. 0.3 vs 0.4).\\n\"\n",
    "    \" 6. Return JSON only.\"\n",
    ")\n",
    "\n",
    "LR_LADDER = (\n",
    "    \"LR guide ➜  >20 pathognomonic · 5-20 very strong for · \"\n",
    "    \"2-5 moderate for · 1.4-2 weak for · 0.71-1.4 neutral · \"\n",
    "    \"0.2-0.71 moderate against · 0.05-0.2 very strong against · <0.05 rule-out.\"\n",
    ")\n",
    "\n",
    "# ----------------  9-SHOT PRIMER  (for gpt-family)  -------------------------\n",
    "RICH_PRIMER = \"\"\"\n",
    "<analysis># bucket: pathognomonic | step 1: prevalence high?\n",
    "C: bacterial meningitis\n",
    "F: nuchal rigidity\n",
    "</analysis>\n",
    "<json>{\"value\": 20}</json>\n",
    "\n",
    "<analysis># bucket: very-strong-for\n",
    "C: pulmonary embolism\n",
    "F: Wells score >6\n",
    "</analysis>\n",
    "<json>{\"value\": 10}</json>\n",
    "\n",
    "<analysis># bucket: strong-for\n",
    "C: ectopic pregnancy\n",
    "F: β-hCG >6500 IU + no intra-uterine sac\n",
    "</analysis>\n",
    "<json>{\"value\": 5}</json>\n",
    "\n",
    "<analysis># bucket: weak-for\n",
    "C: cardiac chest pain\n",
    "F: pain behind sternum\n",
    "</analysis>\n",
    "<json>{\"value\": 1.5}</json>\n",
    "\n",
    "<analysis># bucket: moderate-against\n",
    "C: appendicitis\n",
    "F: guarding absent\n",
    "</analysis>\n",
    "<json>{\"value\": 0.5}</json>\n",
    "\n",
    "<analysis># bucket: very-strong-against\n",
    "C: DKA\n",
    "F: normal anion gap\n",
    "</analysis>\n",
    "<json>{\"value\": 0.1}</json>\n",
    "\n",
    "<analysis># bucket: neutral\n",
    "C: myocardial infarction\n",
    "F: enjoys playing chess\n",
    "</analysis>\n",
    "<json>{\"value\": 1}</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "# --------------  2-SHOT PRIMER  (for o-family)  ----------------------------\n",
    "MINIMAL_PRIMER = \"\"\"\n",
    "<analysis># pathognomonic\n",
    "C: bacterial meningitis\n",
    "F: nuchal rigidity\n",
    "</analysis>\n",
    "<json>{\"value\": 20}</json>\n",
    "\n",
    "<analysis># neutral\n",
    "C: myocardial infarction\n",
    "F: enjoys playing chess\n",
    "</analysis>\n",
    "<json>{\"value\": 1}</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "# ------------ CAPABILITY MAP (single source‑of‑truth for all models) ---------\n",
    "\n",
    "MODEL_CAPABILITIES = {\n",
    "    # family  | length‑field              | temp? | hidden‑token caps to try\n",
    "    \"gpt-4o-mini-2024-07-18\"  : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    \"gpt-4o-2024-08-06\"       : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    \"gpt-4.1-mini-2025-04-14\" : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    \"gpt-4.1-2025-04-14\"      : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    # o‑family – we escalate 512 → 1024 → 2048 if needed\n",
    "    \"o3-mini-2025-01-31\": {\"field\": \"max_completion_tokens\", \"temp\": False,\n",
    "                        \"caps\": [2048]},   # single generous cap\n",
    "    \"o3-2025-04-16\":      {\"field\": \"max_completion_tokens\", \"temp\": False,\n",
    "                        \"caps\": [2048]},\n",
    "    \"o4-mini-2025-04-16\": {\"field\": \"max_completion_tokens\", \"temp\": False,\n",
    "                        \"caps\": [2048]},\n",
    "}\n",
    "\"\"\"\n",
    "# Abbreviated test run\n",
    "MODEL_CAPABILITIES = {\n",
    "    # family  | length‑field              | temp? | hidden‑token caps to try\n",
    "    \"gpt-4o-mini-2024-07-18\"  : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    \"gpt-4.1-mini-2025-04-14\" : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ----------------------------- RESPONSE SCHEMA ------------------------------\n",
    "class LRResponse(BaseModel):\n",
    "    value: float\n",
    "\n",
    "# --------------------------- HELPER FUNCTIONS -------------------------------\n",
    "def build_messages(dx: str, finding: str, for_o: bool) -> list[dict]:\n",
    "    primer = MINIMAL_PRIMER if for_o else RICH_PRIMER\n",
    "    return [\n",
    "        {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"system\",    \"content\": INTERNAL_CHECKLIST},   # ← NEW\n",
    "        {\"role\": \"system\",    \"content\": LR_LADDER},\n",
    "        {\"role\": \"assistant\", \"content\": primer},\n",
    "        {\"role\": \"user\",      \"content\": f\"Condition: {dx}\\nFinding: {finding}\"},\n",
    "    ]\n",
    "\n",
    "def estimate_lr(diagnosis: str, finding: str, client: OpenAI, model: str) -> float:\n",
    "    cfg          = MODEL_CAPABILITIES[model]\n",
    "    is_reasoning = model.startswith(\"o\")\n",
    "\n",
    "    primer      = MINIMAL_PRIMER if is_reasoning else RICH_PRIMER\n",
    "    temperature = 0.20 if is_reasoning else 0.10\n",
    "\n",
    "    for cap in cfg[\"caps\"]:\n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=model,\n",
    "                messages=build_messages(primer, diagnosis, finding),\n",
    "                response_format=LRResponse,\n",
    "                **{cfg[\"field\"]: cap},\n",
    "                **({\"temperature\": temperature} if cfg[\"temp\"] else {}),\n",
    "                **({\"reasoning_effort\": \"medium\"} if model.startswith(\"o3\") else {}),\n",
    "            )\n",
    "            return float(completion.choices[0].message.parsed.value)\n",
    "        except Exception as e:\n",
    "            # Only retry on the length‑limit failure signature\n",
    "            if \"length limit was reached\" in str(e) and cap != cfg[\"caps\"][-1]:\n",
    "                logging.warning(f\"Retrying {model} with larger cap \"\n",
    "                                f\"({cap}→{cfg['caps'][cfg['caps'].index(cap)+1]})\")\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# -------------------------------  MAIN PIPE  --------------------------------\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "models = list(MODEL_CAPABILITIES)\n",
    "input_file, output_file = \"nnt_lrs_processed.xlsx\", \"nnt_lrs_with_estimated.xlsx\"\n",
    "sheets = pd.read_excel(input_file, sheet_name=None, header=None)\n",
    "\n",
    "for sheet_name, df in sheets.items():\n",
    "    diagnosis = df.iloc[0, 0]\n",
    "    for model in models:\n",
    "        new_header, col = \"lr_\" + model, []\n",
    "        print(f\"→ {diagnosis[:60]} | {model}\")\n",
    "        for i in range(len(df)):\n",
    "            if i == 0:           col.append(\"\")\n",
    "            elif i == 1:         col.append(new_header)\n",
    "            else:\n",
    "                try:\n",
    "                    lr = estimate_lr(diagnosis, df.iloc[i, 0], client, model)\n",
    "                except Exception as e:\n",
    "                    lr = \"ERROR\"\n",
    "                    logging.warning(f\"Error on sheet '{sheet_name}', row {i}, \"\n",
    "                                    f\"model {model}: {e}\")\n",
    "                col.append(lr)\n",
    "        df.insert(df.shape[1], new_header, col)\n",
    "    sheets[sheet_name] = df\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    for name, frame in sheets.items():\n",
    "        frame.to_excel(writer, sheet_name=name, index=False, header=False)\n",
    "\n",
    "print(f\"✅  All done – results saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newest Version (Aug 22nd)\n",
    "\n",
    "GPT-5 Capabilities added and revised prompting strategy mildly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-5\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-5-mini\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-5-nano\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4.1-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4.1-mini-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4.1-nano-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4o-2024-11-20\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4o-mini-2024-07-18\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | o3-2025-04-16\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | o3-mini-2025-01-31\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | o4-mini-2025-04-16\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-5\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-5-mini\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-5-nano\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-4.1-2025-04-14\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-4.1-mini-2025-04-14\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-4.1-nano-2025-04-14\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-4o-2024-11-20\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-4o-mini-2024-07-18\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | o3-2025-04-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Error on sheet 'fficult Endotracheal Intubation', row 12, model o3-2025-04-16: Connection error.\n",
      "WARNING:root:Error on sheet 'fficult Endotracheal Intubation', row 13, model o3-2025-04-16: Connection error.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Factors Predicting Difficult Endotracheal Intubation | o3-mini-2025-01-31\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | o4-mini-2025-04-16\n",
      "→ Acute Coronary Syndrome | gpt-5\n",
      "→ Acute Coronary Syndrome | gpt-5-mini\n",
      "→ Acute Coronary Syndrome | gpt-5-nano\n",
      "→ Acute Coronary Syndrome | gpt-4.1-2025-04-14\n",
      "→ Acute Coronary Syndrome | gpt-4.1-mini-2025-04-14\n",
      "→ Acute Coronary Syndrome | gpt-4.1-nano-2025-04-14\n",
      "→ Acute Coronary Syndrome | gpt-4o-2024-11-20\n",
      "→ Acute Coronary Syndrome | gpt-4o-mini-2024-07-18\n",
      "→ Acute Coronary Syndrome | o3-2025-04-16\n",
      "→ Acute Coronary Syndrome | o3-mini-2025-01-31\n",
      "→ Acute Coronary Syndrome | o4-mini-2025-04-16\n",
      "→ Aortic Dissection | gpt-5\n",
      "→ Aortic Dissection | gpt-5-mini\n",
      "→ Aortic Dissection | gpt-5-nano\n",
      "→ Aortic Dissection | gpt-4.1-2025-04-14\n",
      "→ Aortic Dissection | gpt-4.1-mini-2025-04-14\n",
      "→ Aortic Dissection | gpt-4.1-nano-2025-04-14\n",
      "→ Aortic Dissection | gpt-4o-2024-11-20\n",
      "→ Aortic Dissection | gpt-4o-mini-2024-07-18\n",
      "→ Aortic Dissection | o3-2025-04-16\n",
      "→ Aortic Dissection | o3-mini-2025-01-31\n",
      "→ Aortic Dissection | o4-mini-2025-04-16\n",
      "→ Deep Venous Thrombosis (DVT) | gpt-5\n",
      "→ Deep Venous Thrombosis (DVT) | gpt-5-mini\n",
      "→ Deep Venous Thrombosis (DVT) | gpt-5-nano\n",
      "→ Deep Venous Thrombosis (DVT) | gpt-4.1-2025-04-14\n",
      "→ Deep Venous Thrombosis (DVT) | gpt-4.1-mini-2025-04-14\n",
      "→ Deep Venous Thrombosis (DVT) | gpt-4.1-nano-2025-04-14\n",
      "→ Deep Venous Thrombosis (DVT) | gpt-4o-2024-11-20\n",
      "→ Deep Venous Thrombosis (DVT) | gpt-4o-mini-2024-07-18\n",
      "→ Deep Venous Thrombosis (DVT) | o3-2025-04-16\n",
      "→ Deep Venous Thrombosis (DVT) | o3-mini-2025-01-31\n",
      "→ Deep Venous Thrombosis (DVT) | o4-mini-2025-04-16\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | gpt-5\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | gpt-5-mini\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | gpt-5-nano\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | gpt-4.1-2025-04-14\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | gpt-4.1-mini-2025-04-14\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | gpt-4.1-nano-2025-04-14\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | gpt-4o-2024-11-20\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | gpt-4o-mini-2024-07-18\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | o3-2025-04-16\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | o3-mini-2025-01-31\n",
      "→ Dyspnea Due to Acute Heart Failure Syndrome | o4-mini-2025-04-16\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | gpt-5\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | gpt-5-mini\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | gpt-5-nano\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | gpt-4.1-2025-04-14\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | gpt-4.1-mini-2025-04-14\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | gpt-4.1-nano-2025-04-14\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | gpt-4o-2024-11-20\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | gpt-4o-mini-2024-07-18\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | o3-2025-04-16\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | o3-mini-2025-01-31\n",
      "→ Dyspnea Due to Heart Failure (With Chronic Respiratory Disea | o4-mini-2025-04-16\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | gpt-5\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | gpt-5-mini\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | gpt-5-nano\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | gpt-4.1-2025-04-14\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | gpt-4.1-mini-2025-04-14\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | gpt-4.1-nano-2025-04-14\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | gpt-4o-2024-11-20\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | gpt-4o-mini-2024-07-18\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | o3-2025-04-16\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | o3-mini-2025-01-31\n",
      "→ Dyspnea Due to Heart Failure (Without Chronic Respiratory Di | o4-mini-2025-04-16\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | gpt-5\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | gpt-5-mini\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | gpt-5-nano\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | gpt-4.1-2025-04-14\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | gpt-4.1-mini-2025-04-14\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | gpt-4.1-nano-2025-04-14\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | gpt-4o-2024-11-20\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | gpt-4o-mini-2024-07-18\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | o3-2025-04-16\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | o3-mini-2025-01-31\n",
      "→ Markers of Fluid Responsiveness in Hemodynamically Unstable  | o4-mini-2025-04-16\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | gpt-5\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | gpt-5-mini\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | gpt-5-nano\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | gpt-4.1-2025-04-14\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | gpt-4.1-mini-2025-04-14\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | gpt-4.1-nano-2025-04-14\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | gpt-4o-2024-11-20\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | gpt-4o-mini-2024-07-18\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | o3-2025-04-16\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | o3-mini-2025-01-31\n",
      "→ Use of the Clinical Examination in the Diagnosis of Cardiac  | o4-mini-2025-04-16\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | gpt-5\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | gpt-5-mini\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | gpt-5-nano\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | gpt-4.1-2025-04-14\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | gpt-4.1-mini-2025-04-14\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | gpt-4.1-nano-2025-04-14\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | gpt-4o-2024-11-20\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | gpt-4o-mini-2024-07-18\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | o3-2025-04-16\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | o3-mini-2025-01-31\n",
      "→ Accuracy of Physical Examination and Imaging Findings for th | o4-mini-2025-04-16\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | gpt-5\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | gpt-5-mini\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | gpt-5-nano\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | gpt-4.1-2025-04-14\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | gpt-4.1-mini-2025-04-14\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | gpt-4.1-nano-2025-04-14\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | gpt-4o-2024-11-20\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | gpt-4o-mini-2024-07-18\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | o3-2025-04-16\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | o3-mini-2025-01-31\n",
      "→ Ankle-Brachial Index for Diagnosis of Arterial Injury in Pen | o4-mini-2025-04-16\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | gpt-5\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | gpt-5-mini\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | gpt-5-nano\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | gpt-4.1-2025-04-14\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | gpt-4.1-mini-2025-04-14\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | gpt-4.1-nano-2025-04-14\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | gpt-4o-2024-11-20\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | gpt-4o-mini-2024-07-18\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | o3-2025-04-16\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | o3-mini-2025-01-31\n",
      "→ Diagnostic Accuracy of Point-of-Care Ultrasound for Retinal  | o4-mini-2025-04-16\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | gpt-5\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | gpt-5-mini\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | gpt-5-nano\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | gpt-4.1-2025-04-14\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | gpt-4.1-mini-2025-04-14\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | gpt-4.1-nano-2025-04-14\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | gpt-4o-2024-11-20\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | gpt-4o-mini-2024-07-18\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | o3-2025-04-16\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | o3-mini-2025-01-31\n",
      "→ Diagnostic Accuracy of the History, Physical Examination, an | o4-mini-2025-04-16\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | gpt-5\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | gpt-5-mini\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | gpt-5-nano\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | gpt-4.1-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | gpt-4.1-mini-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | gpt-4.1-nano-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | gpt-4o-2024-11-20\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | gpt-4o-mini-2024-07-18\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | o3-2025-04-16\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | o3-mini-2025-01-31\n",
      "→ Diagnostic Accuracy of Ultrasound for the Evaluation of Smal | o4-mini-2025-04-16\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | gpt-5\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | gpt-5-mini\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | gpt-5-nano\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | gpt-4.1-2025-04-14\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | gpt-4.1-mini-2025-04-14\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | gpt-4.1-nano-2025-04-14\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | gpt-4o-2024-11-20\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | gpt-4o-mini-2024-07-18\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | o3-2025-04-16\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | o3-mini-2025-01-31\n",
      "→ Operating Characteristics of Diagnostic Tests for Syphilis | o4-mini-2025-04-16\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | gpt-5\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | gpt-5-mini\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | gpt-5-nano\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | gpt-4.1-2025-04-14\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | gpt-4.1-mini-2025-04-14\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | gpt-4.1-nano-2025-04-14\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | gpt-4o-2024-11-20\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | gpt-4o-mini-2024-07-18\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | o3-2025-04-16\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | o3-mini-2025-01-31\n",
      "→ Lung Ultrasound for Diagnosis of Pneumonia in Children | o4-mini-2025-04-16\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | gpt-5\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | gpt-5-mini\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | gpt-5-nano\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | gpt-4.1-2025-04-14\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | gpt-4.1-mini-2025-04-14\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | gpt-4.1-nano-2025-04-14\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | gpt-4o-2024-11-20\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | gpt-4o-mini-2024-07-18\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | o3-2025-04-16\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | o3-mini-2025-01-31\n",
      "→ Point-of-Care Ultrasound for the Diagnosis of Thoracoabdomin | o4-mini-2025-04-16\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | gpt-5\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | gpt-5-mini\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | gpt-5-nano\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | gpt-4.1-2025-04-14\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | gpt-4.1-mini-2025-04-14\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | gpt-4.1-nano-2025-04-14\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | gpt-4o-2024-11-20\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | gpt-4o-mini-2024-07-18\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | o3-2025-04-16\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | o3-mini-2025-01-31\n",
      "→ Retinal Pathology in Patients with Acute Onset Flashes and F | o4-mini-2025-04-16\n",
      "→ Hypovolemia | gpt-5\n",
      "→ Hypovolemia | gpt-5-mini\n",
      "→ Hypovolemia | gpt-5-nano\n",
      "→ Hypovolemia | gpt-4.1-2025-04-14\n",
      "→ Hypovolemia | gpt-4.1-mini-2025-04-14\n",
      "→ Hypovolemia | gpt-4.1-nano-2025-04-14\n",
      "→ Hypovolemia | gpt-4o-2024-11-20\n",
      "→ Hypovolemia | gpt-4o-mini-2024-07-18\n",
      "→ Hypovolemia | o3-2025-04-16\n",
      "→ Hypovolemia | o3-mini-2025-01-31\n",
      "→ Hypovolemia | o4-mini-2025-04-16\n",
      "→ Malaria in Returning Travelers | gpt-5\n",
      "→ Malaria in Returning Travelers | gpt-5-mini\n",
      "→ Malaria in Returning Travelers | gpt-5-nano\n",
      "→ Malaria in Returning Travelers | gpt-4.1-2025-04-14\n",
      "→ Malaria in Returning Travelers | gpt-4.1-mini-2025-04-14\n",
      "→ Malaria in Returning Travelers | gpt-4.1-nano-2025-04-14\n",
      "→ Malaria in Returning Travelers | gpt-4o-2024-11-20\n",
      "→ Malaria in Returning Travelers | gpt-4o-mini-2024-07-18\n",
      "→ Malaria in Returning Travelers | o3-2025-04-16\n",
      "→ Malaria in Returning Travelers | o3-mini-2025-01-31\n",
      "→ Malaria in Returning Travelers | o4-mini-2025-04-16\n",
      "→ Osteomyelitis in Diabetic Patients | gpt-5\n",
      "→ Osteomyelitis in Diabetic Patients | gpt-5-mini\n",
      "→ Osteomyelitis in Diabetic Patients | gpt-5-nano\n",
      "→ Osteomyelitis in Diabetic Patients | gpt-4.1-2025-04-14\n",
      "→ Osteomyelitis in Diabetic Patients | gpt-4.1-mini-2025-04-14\n",
      "→ Osteomyelitis in Diabetic Patients | gpt-4.1-nano-2025-04-14\n",
      "→ Osteomyelitis in Diabetic Patients | gpt-4o-2024-11-20\n",
      "→ Osteomyelitis in Diabetic Patients | gpt-4o-mini-2024-07-18\n",
      "→ Osteomyelitis in Diabetic Patients | o3-2025-04-16\n",
      "→ Osteomyelitis in Diabetic Patients | o3-mini-2025-01-31\n",
      "→ Osteomyelitis in Diabetic Patients | o4-mini-2025-04-16\n",
      "→ Pertussis (Whooping Cough) | gpt-5\n",
      "→ Pertussis (Whooping Cough) | gpt-5-mini\n",
      "→ Pertussis (Whooping Cough) | gpt-5-nano\n",
      "→ Pertussis (Whooping Cough) | gpt-4.1-2025-04-14\n",
      "→ Pertussis (Whooping Cough) | gpt-4.1-mini-2025-04-14\n",
      "→ Pertussis (Whooping Cough) | gpt-4.1-nano-2025-04-14\n",
      "→ Pertussis (Whooping Cough) | gpt-4o-2024-11-20\n",
      "→ Pertussis (Whooping Cough) | gpt-4o-mini-2024-07-18\n",
      "→ Pertussis (Whooping Cough) | o3-2025-04-16\n",
      "→ Pertussis (Whooping Cough) | o3-mini-2025-01-31\n",
      "→ Pertussis (Whooping Cough) | o4-mini-2025-04-16\n",
      "→ Streptococal Pharyngitis | gpt-5\n",
      "→ Streptococal Pharyngitis | gpt-5-mini\n",
      "→ Streptococal Pharyngitis | gpt-5-nano\n",
      "→ Streptococal Pharyngitis | gpt-4.1-2025-04-14\n",
      "→ Streptococal Pharyngitis | gpt-4.1-mini-2025-04-14\n",
      "→ Streptococal Pharyngitis | gpt-4.1-nano-2025-04-14\n",
      "→ Streptococal Pharyngitis | gpt-4o-2024-11-20\n",
      "→ Streptococal Pharyngitis | gpt-4o-mini-2024-07-18\n",
      "→ Streptococal Pharyngitis | o3-2025-04-16\n",
      "→ Streptococal Pharyngitis | o3-mini-2025-01-31\n",
      "→ Streptococal Pharyngitis | o4-mini-2025-04-16\n",
      "→ Hemorrhagic Stroke | gpt-5\n",
      "→ Hemorrhagic Stroke | gpt-5-mini\n",
      "→ Hemorrhagic Stroke | gpt-5-nano\n",
      "→ Hemorrhagic Stroke | gpt-4.1-2025-04-14\n",
      "→ Hemorrhagic Stroke | gpt-4.1-mini-2025-04-14\n",
      "→ Hemorrhagic Stroke | gpt-4.1-nano-2025-04-14\n",
      "→ Hemorrhagic Stroke | gpt-4o-2024-11-20\n",
      "→ Hemorrhagic Stroke | gpt-4o-mini-2024-07-18\n",
      "→ Hemorrhagic Stroke | o3-2025-04-16\n",
      "→ Hemorrhagic Stroke | o3-mini-2025-01-31\n",
      "→ Hemorrhagic Stroke | o4-mini-2025-04-16\n",
      "→ Migraine | gpt-5\n",
      "→ Migraine | gpt-5-mini\n",
      "→ Migraine | gpt-5-nano\n",
      "→ Migraine | gpt-4.1-2025-04-14\n",
      "→ Migraine | gpt-4.1-mini-2025-04-14\n",
      "→ Migraine | gpt-4.1-nano-2025-04-14\n",
      "→ Migraine | gpt-4o-2024-11-20\n",
      "→ Migraine | gpt-4o-mini-2024-07-18\n",
      "→ Migraine | o3-2025-04-16\n",
      "→ Migraine | o3-mini-2025-01-31\n",
      "→ Migraine | o4-mini-2025-04-16\n",
      "→ Myasthenia Gravis | gpt-5\n",
      "→ Myasthenia Gravis | gpt-5-mini\n",
      "→ Myasthenia Gravis | gpt-5-nano\n",
      "→ Myasthenia Gravis | gpt-4.1-2025-04-14\n",
      "→ Myasthenia Gravis | gpt-4.1-mini-2025-04-14\n",
      "→ Myasthenia Gravis | gpt-4.1-nano-2025-04-14\n",
      "→ Myasthenia Gravis | gpt-4o-2024-11-20\n",
      "→ Myasthenia Gravis | gpt-4o-mini-2024-07-18\n",
      "→ Myasthenia Gravis | o3-2025-04-16\n",
      "→ Myasthenia Gravis | o3-mini-2025-01-31\n",
      "→ Myasthenia Gravis | o4-mini-2025-04-16\n",
      "→ Spinal Stenosis in the Elderly | gpt-5\n",
      "→ Spinal Stenosis in the Elderly | gpt-5-mini\n",
      "→ Spinal Stenosis in the Elderly | gpt-5-nano\n",
      "→ Spinal Stenosis in the Elderly | gpt-4.1-2025-04-14\n",
      "→ Spinal Stenosis in the Elderly | gpt-4.1-mini-2025-04-14\n",
      "→ Spinal Stenosis in the Elderly | gpt-4.1-nano-2025-04-14\n",
      "→ Spinal Stenosis in the Elderly | gpt-4o-2024-11-20\n",
      "→ Spinal Stenosis in the Elderly | gpt-4o-mini-2024-07-18\n",
      "→ Spinal Stenosis in the Elderly | o3-2025-04-16\n",
      "→ Spinal Stenosis in the Elderly | o3-mini-2025-01-31\n",
      "→ Spinal Stenosis in the Elderly | o4-mini-2025-04-16\n",
      "→ Temporal Arteritis | gpt-5\n",
      "→ Temporal Arteritis | gpt-5-mini\n",
      "→ Temporal Arteritis | gpt-5-nano\n",
      "→ Temporal Arteritis | gpt-4.1-2025-04-14\n",
      "→ Temporal Arteritis | gpt-4.1-mini-2025-04-14\n",
      "→ Temporal Arteritis | gpt-4.1-nano-2025-04-14\n",
      "→ Temporal Arteritis | gpt-4o-2024-11-20\n",
      "→ Temporal Arteritis | gpt-4o-mini-2024-07-18\n",
      "→ Temporal Arteritis | o3-2025-04-16\n",
      "→ Temporal Arteritis | o3-mini-2025-01-31\n",
      "→ Temporal Arteritis | o4-mini-2025-04-16\n",
      "→ Carpal Tunnel Syndrome | gpt-5\n",
      "→ Carpal Tunnel Syndrome | gpt-5-mini\n",
      "→ Carpal Tunnel Syndrome | gpt-5-nano\n",
      "→ Carpal Tunnel Syndrome | gpt-4.1-2025-04-14\n",
      "→ Carpal Tunnel Syndrome | gpt-4.1-mini-2025-04-14\n",
      "→ Carpal Tunnel Syndrome | gpt-4.1-nano-2025-04-14\n",
      "→ Carpal Tunnel Syndrome | gpt-4o-2024-11-20\n",
      "→ Carpal Tunnel Syndrome | gpt-4o-mini-2024-07-18\n",
      "→ Carpal Tunnel Syndrome | o3-2025-04-16\n",
      "→ Carpal Tunnel Syndrome | o3-mini-2025-01-31\n",
      "→ Carpal Tunnel Syndrome | o4-mini-2025-04-16\n",
      "Done – results saved to 'nnt_lrs_with_estimated.xlsx'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bayesian LR estimator — Responses API + GPT‑5–aware (no max_output_tokens)\n",
    "Updated: 2025‑08‑22\n",
    "\n",
    "Changes vs. your prior block:\n",
    "• Responses API only (no Chat fallback).\n",
    "• No max_output_tokens (output is schema‑constrained and tiny).\n",
    "• Reasoning models use reasoning.effort=\"medium\"; no temperature/top_p.\n",
    "• GPT‑5(+mini) use text.verbosity=\"low\" (applied only where supported).\n",
    "• Includes these models: gpt‑5, gpt‑5‑mini, gpt‑4o‑mini‑2024‑07‑18, gpt‑4o‑2024‑08‑06,\n",
    "  gpt‑4.1‑mini‑2025‑04‑14, gpt‑4.1‑2025‑04‑14, o3‑mini‑2025‑01‑31, o3‑2025‑04‑16, o4‑mini‑2025‑04‑16.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import time, math\n",
    "from random import random\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Model registry: whether it’s a reasoning model, whether it supports verbosity,\n",
    "# and whether temperature is allowed.\n",
    "\n",
    "MODEL_CAPABILITIES = {\n",
    "    # GPT‑5 series (reasoning; supports text.verbosity; no temperature)\n",
    "    \"gpt-5\"        : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "    \"gpt-5-mini\"   : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "    \"gpt-5-nano\"   : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "\n",
    "    # GPT‑4.1 family (non‑reasoning; temperature OK); include snapshots + aliases\n",
    "    \"gpt-4.1-2025-04-14\" : {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4.1-mini-2025-04-14\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4.1-nano-2025-04-14\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "\n",
    "    # GPT‑4o family (non‑reasoning; temperature OK); prefer latest snapshot or alias\n",
    "    \"gpt-4o-2024-11-20\"  : {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4o-mini-2024-07-18\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "\n",
    "    # o‑series (reasoning; no temperature)\n",
    "    \"o3-2025-04-16\" : {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "    \"o3-mini-2025-01-31\": {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "    \"o4-mini-2025-04-16\": {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "}\n",
    "MODELS = list(MODEL_CAPABILITIES)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "INPUT_FILE  = \"nnt_lrs_processed.xlsx\"\n",
    "OUTPUT_FILE = \"nnt_lrs_with_estimated.xlsx\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Prompt (concise; no chain‑of‑thought)\n",
    "# -----------------------------------------------------------------------------\n",
    "SYSTEM_CORE = \"\"\"You are a Bayesian diagnostic assistant.\n",
    "Estimate a numeric likelihood ratio (LR) for a finding with respect to a diagnosis.\n",
    "Return only a JSON object matching the schema: {\"value\": <float>}, where value > 0.\n",
    "\"\"\"\n",
    "\n",
    "DEFINITION = \"\"\"Definition:\n",
    "LR = P(finding | diagnosis) / P(finding | not-diagnosis)\n",
    "\"\"\"\n",
    "\n",
    "BANDS = \"\"\"Reference bands (orientation):\n",
    ">10 strong for; 5-10 moderate for; 2–5 weak for;\n",
    "0.5–2 negligible;\n",
    "0.2-0.5 weak against; 0.1-0.2 moderate against; ≤0.1 strong against\"\"\"\n",
    "\n",
    "# Few‑shot examples (assistant outputs are JSON only)\n",
    "FEW_SHOT_RICH = [\n",
    "    (\"subarachnoid hemorrhage\", \"xanthochromia present in CSF\",                     19.0),\n",
    "    (\"pericarditis\",            \"pleuritic chest pain improved by leaning forward\",  5.2),\n",
    "    (\"pulmonary embolism\",      \"tachycardia >100 bpm\",                              2.2),\n",
    "    (\"urinary tract infection\", \"malodorous urine\",                                  1.1),\n",
    "    (\"myocardial infarction\",  \"enjoys playing chess\",                               1.0),\n",
    "    (\"appendicitis\",            \"no RLQ tenderness\",                                0.45),\n",
    "    (\"pneumothorax\",            \"bilateral lung sliding present on US\",             0.18),\n",
    "    (\"HIV infection\",           \"4th‑generation Ag/Ab screen negative beyond window\",0.05),\n",
    "\n",
    "]\n",
    "\n",
    "FEW_SHOT_MIN = [\n",
    "    (\"subarachnoid hemorrhage\", \"xanthochromia present in CSF\",                    19.0),\n",
    "    (\"myocardial infarction\",  \"enjoys playing chess\",                             1.0),\n",
    "]\n",
    "\n",
    "def build_messages(diagnosis: str, finding: str, reasoning: bool) -> list[dict]:\n",
    "    msgs: list[dict] = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_CORE.strip()},\n",
    "        {\"role\": \"system\", \"content\": DEFINITION.strip()},\n",
    "        {\"role\": \"system\", \"content\": BANDS.strip()},\n",
    "    ]\n",
    "    examples = FEW_SHOT_MIN if reasoning else FEW_SHOT_RICH\n",
    "    for dx_ex, f_ex, v_ex in examples:\n",
    "        msgs.append({\"role\": \"user\",      \"content\": f\"Condition: {dx_ex}\\nFinding: {f_ex}\"})\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": f'{{\"value\": {float(v_ex)}}}'})\n",
    "    msgs.append({\"role\": \"user\", \"content\": f\"Condition: {diagnosis}\\nFinding: {finding}\"})\n",
    "    return msgs\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Structured Outputs schema (Pydantic)\n",
    "# -----------------------------------------------------------------------------\n",
    "class LRResponse(BaseModel):\n",
    "    value: float\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2b) Retry wrapper (exponential backoff with jitter)\n",
    "# -----------------------------------------------------------------------------\n",
    "def estimate_lr_until_positive(\n",
    "    diagnosis: str,\n",
    "    finding: str,\n",
    "    model: str,\n",
    "    client: Optional[OpenAI] = None,\n",
    "    max_retries: Optional[int] = None,      # None ⇒ retry indefinitely\n",
    "    base_backoff: float = 0.5,              # seconds\n",
    "    max_backoff: float = 30.0               # seconds\n",
    ") -> float:\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            lr = estimate_lr(diagnosis, finding, model, client)\n",
    "            if isinstance(lr, (int, float)) and math.isfinite(lr) and lr > 0:\n",
    "                return float(lr)\n",
    "            raise ValueError(f\"Non‑positive or non‑finite LR: {lr!r}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(\n",
    "                f\"[retry {attempt}] sheet finding='{finding[:80]}' | \"\n",
    "                f\"model={model} → {e}\"\n",
    "            )\n",
    "            if (max_retries is not None) and (attempt >= max_retries):\n",
    "                raise\n",
    "            # exponential backoff with jitter\n",
    "            delay = min(base_backoff * (2 ** (attempt - 1)), max_backoff)\n",
    "            time.sleep(delay * (0.5 + random()))  # 0.5–1.5× jitter\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) One‑call estimator (Responses API)\n",
    "# -----------------------------------------------------------------------------\n",
    "def estimate_lr(diagnosis: str, finding: str, model: str, client: Optional[OpenAI] = None) -> float:\n",
    "    if client is None:\n",
    "        client = OpenAI()\n",
    "\n",
    "    cfg = MODEL_CAPABILITIES[model]\n",
    "    msgs = build_messages(diagnosis, finding, reasoning=cfg[\"reasoning\"])\n",
    "\n",
    "    kwargs = {}\n",
    "    if cfg[\"reasoning\"]:\n",
    "        kwargs[\"reasoning\"] = {\"effort\": \"medium\"}     # for GPT‑5 and o‑series\n",
    "        # no temperature/top_p\n",
    "    elif cfg[\"allow_temp\"]:\n",
    "        kwargs[\"temperature\"] = 0.2                    # allowed for 4o / 4.1\n",
    "\n",
    "    # Apply verbosity only where supported (GPT‑5 family)\n",
    "    if cfg[\"verbosity\"]:\n",
    "        kwargs[\"text\"] = {\"verbosity\": \"low\"}\n",
    "\n",
    "    resp = client.responses.parse(\n",
    "        model=model,\n",
    "        input=msgs,\n",
    "        text_format=LRResponse,    # Structured Outputs → Pydantic\n",
    "        **kwargs,\n",
    "    )\n",
    "    return float(resp.output_parsed.value)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Main pipeline: read workbook → append model columns → write output\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_batch(input_file: str | Path, output_file: str | Path, models: list[str]) -> None:\n",
    "    sheets = pd.read_excel(input_file, sheet_name=None, header=None)\n",
    "\n",
    "    for sheet_name, df in sheets.items():\n",
    "        diagnosis = str(df.iloc[0, 0]).strip()\n",
    "        for model in models:\n",
    "            new_header = \"lr_\" + model\n",
    "            col = []\n",
    "            print(f\"→ {diagnosis[:60]} | {model}\")\n",
    "            for i in range(len(df)):\n",
    "                if i == 0:\n",
    "                    col.append(\"\")                   # top-left cell (sheet label row)\n",
    "                elif i == 1:\n",
    "                    col.append(new_header)           # column header row\n",
    "                else:\n",
    "                    finding = str(df.iloc[i, 0]).strip()\n",
    "                    if not finding:\n",
    "                        col.append(\"\")               # keep blank rows blank\n",
    "                        continue\n",
    "                    try:\n",
    "                        # retry until a strictly positive, finite float is returned\n",
    "                        lr = estimate_lr_until_positive(\n",
    "                            diagnosis, finding, model, client,\n",
    "                            max_retries=None         # set to an int (e.g., 8) to cap retries\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        lr = \"ERROR\"\n",
    "                        logging.warning(\n",
    "                            f\"Error on sheet '{sheet_name}', row {i}, model {model} after retries: {e}\"\n",
    "                        )\n",
    "                    col.append(lr)\n",
    "            # Insert as object dtype to accommodate strings like \"ERROR\"\n",
    "            df.insert(df.shape[1], new_header, pd.Series(col, dtype=\"object\"))\n",
    "        sheets[sheet_name] = df\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        for name, frame in sheets.items():\n",
    "            frame.to_excel(writer, sheet_name=name, index=False, header=False)\n",
    "\n",
    "    print(f\"Done – results saved to '{output_file}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_batch(INPUT_FILE, OUTPUT_FILE, MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block to generate the processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅  Converted workbook written to: new_NNT_LRs_08-23-2025.xlsx (single sheet: 'Master')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1.  Optional helper – keep the same “Feature Type” buckets   \n",
    "#  [ ] TODO: this does not seem like it works very well - just usual manual for now #\n",
    "# ------------------------------------------------------------------ #\n",
    "def classify_feature_type(text: str) -> str:\n",
    "    \"\"\"Heuristic that matches the legacy categories.\"\"\"\n",
    "    t = str(text).lower()\n",
    "\n",
    "    history = \"history:\" in t\n",
    "    sign    = (\"sign:\" in t) or (\"symptom\" in t)\n",
    "    score   = any(k in t for k in (\"score\", \"points\", \"rule\"))\n",
    "    test    = any(k in t for k in (\"test:\", \"lab\", \"troponin\", \"d‑dimer\"))\n",
    "    img     = any(k in t for k in (\n",
    "        \"ultrasound\", \"ct\", \"mri\", \"x‑ray\", \"radiograph\", \"imaging\",\n",
    "        \"echo\", \"angiogram\"))\n",
    "\n",
    "    if history and test: return \"History and Test\"\n",
    "    if history and img:  return \"History and imaging\"\n",
    "    if history:          return \"History_\"\n",
    "    if sign:             return \"Sign_symptom\"\n",
    "    if score:            return \"Score\"\n",
    "    if test:             return \"Test finding\"\n",
    "    if img:              return \"Imaging finding\"\n",
    "    return \"Diagnosis\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2.  Converter – writes **one** sheet (same name as in template)     #\n",
    "# ------------------------------------------------------------------ #\n",
    "def convert_lr_workbook(\n",
    "    *,                       # keyword‑only for clarity\n",
    "    input_file: str | Path,  # e.g. \"nnt_lrs_with_estimated.xlsx\"\n",
    "    template_file: str | Path,  # updated example workbook\n",
    "    output_file: str | Path = \"nnt_lrs_converted.xlsx\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    • Collapses all per‑condition tabs from `input_file` into a master frame.\n",
    "    • Adds the right‑most `condition` column (original tab name).\n",
    "    • Writes a single worksheet whose name matches the (only) sheet\n",
    "      found in `template_file`, with the header row stored as **row 1**\n",
    "      (no Excel column headers) to preserve downstream‑notebook compatibility.\n",
    "    \"\"\"\n",
    "    input_file    = Path(input_file)\n",
    "    template_file = Path(template_file)\n",
    "    output_file   = Path(output_file)\n",
    "\n",
    "    # -------- determine the sole sheet name from the template --------------\n",
    "    template_xls = pd.ExcelFile(template_file, engine=\"openpyxl\")\n",
    "    if len(template_xls.sheet_names) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Template has {len(template_xls.sheet_names)} sheets; \"\n",
    "            \"expected exactly one after pruning.\"\n",
    "        )\n",
    "    target_sheet = template_xls.sheet_names[0]  # e.g. \"Master\"\n",
    "\n",
    "    # ------------------- build master dataframe ----------------------------\n",
    "    in_xls = pd.ExcelFile(input_file, engine=\"openpyxl\")\n",
    "    frames = []\n",
    "\n",
    "    for tab in in_xls.sheet_names:\n",
    "        raw = pd.read_excel(in_xls, sheet_name=tab, header=None)\n",
    "\n",
    "        # Expect: row‑0 = diagnosis, row‑1 = column labels, row‑2+ = data\n",
    "        if raw.shape[0] < 3:\n",
    "            continue  # skip empty or malformed tabs\n",
    "\n",
    "        header = raw.iloc[1]\n",
    "        df = raw.iloc[2:].copy()\n",
    "        df.columns = header\n",
    "        df = df.loc[:, ~df.columns.isna()]  # drop unnamed columns\n",
    "\n",
    "        # back‑fill Feature Type if the sheet didn't have it\n",
    "        if \"Feature Type\" not in df.columns:\n",
    "            df[\"Feature Type\"] = df.iloc[:, 0].apply(classify_feature_type)\n",
    "\n",
    "        df[\"condition\"] = tab          # new right‑most column\n",
    "        frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(\"No valid data found in the input workbook.\")\n",
    "\n",
    "    master = pd.concat(frames, ignore_index=True)\n",
    "    # ensure 'condition' is the final column\n",
    "    master = master[[c for c in master.columns if c != \"condition\"] + [\"condition\"]]\n",
    "\n",
    "    # ------------------------ write single sheet ---------------------------\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        # replicate legacy layout: header row lives **inside** the block\n",
    "        block = pd.concat(\n",
    "            [pd.DataFrame([master.columns], columns=master.columns), master],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        block.to_excel(\n",
    "            writer,\n",
    "            sheet_name=target_sheet[:31],  # Excel 31‑char cap\n",
    "            index=False,\n",
    "            header=False,\n",
    "        )\n",
    "\n",
    "    print(f\"✅  Converted workbook written to: {output_file} \"\n",
    "          f\"(single sheet: '{target_sheet}')\")\n",
    "    \n",
    "\n",
    "# Run the conversion\n",
    "convert_lr_workbook(\n",
    "    input_file   = 'nnt_lrs_with_estimated.xlsx',   # produced after LR loop\n",
    "    template_file= 'Past Runs/example_NNT_LRs_PC_07.03.2025.xlsx',  # updated example\n",
    "    output_file  = 'new_NNT_LRs_08-23-2025.xlsx',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block for trouble shooting the Scraper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# url = 'https://thennt.com/lr/dyspnea-due-to-heart-failure-without-chronic-respiratory-disease/'\n",
    "url = 'https://thennt.com/lr/diagnostic-accuracy-history-physical-examination-laboratory-testing-giant-cell-arteritis/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
