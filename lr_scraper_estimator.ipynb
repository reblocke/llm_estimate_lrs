{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Scraper and Estimator\n",
    "\n",
    "This notebook contains: \n",
    "\n",
    "1. code to scrape diagnostic likelihood ratios from theNNT.com and convert them to numerical form. \n",
    "2. code to generate prompts for large language models to estimate the likelihood ratios, then calls to the OpenAI API. \n",
    "\n",
    "The initial output is a spreadsheet called: nnt_lrs_with_estimated which contains: \n",
    "- a sheet for each diagnosis or prediction target\n",
    "- a row for each piece of information\n",
    "- columns for the name, raw nnt lr, processed nnt lr, and estimated by 1 or more LLMs\n",
    "\n",
    "Then, there is a final processing step to collate the reported and estimated LRs into a single sheet to facilitate data analysis, manually named NNT_LRs_08-26-2025.xlsx. \n",
    "\n",
    "For manuscript quality reference standard data-set, manual cleaning supplemented this automated extraction pipeline (tasks noted in workbook). \n",
    "\n",
    "- The un-manually cleaned version is nnt_lrs_processed_without_manual.xlsx\n",
    "- the manually cleaned version, which is used for the manuscript, is nnt_lrs_processed.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI  # or your appropriate client wrapper\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # looks for a .env file (in the current dir by default) that contains an OPENAI_API_KEY=\"\" variable\n",
    "#print(os.getenv(\"OPENAI_API_KEY\"))  # don't upload your openai key to github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data from the NNT \n",
    "\n",
    "This scrapes all of the likelihood ratios listed on the NNT ('https://thennt.com/home-lr/') into Excel spreadsheets. \n",
    "\n",
    "1. A spreadsheet (\"nnt_lrs.xlsx\") contains a separate sheet for each page, which corresponds to a \"prediction tasks\" e.g. diagnosing the cause of a symptom - sometimes with specification of an intended population. Each sheet contains two columns: the name of the features (e.g. test result, finding, historical occurence, comorbditiy), the second contains the raw listing from the spreadsheet\n",
    "\n",
    "2. A second spreadsheet contains the same sheets corresponding to a prediction target, and all of the features. These are used two call the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specialty_links():\n",
    "    \"\"\"\n",
    "    Extracts specialties and their corresponding article links from the webpage.\n",
    "    Returns a list of dictionaries with specialty names and associated links.\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://thennt.com/home-lr/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the section with \"Diagnosis (LR) Reviews by Specialty\"\n",
    "    specialty_section = soup.find('div', class_='well subdisplay accordion_caption', id='lr-byspecialty')\n",
    "\n",
    "    if not specialty_section:\n",
    "        print(\"Could not find the 'Diagnosis (LR) Reviews by Specialty' section on the webpage.\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Find all specialty headings (e.g., h3)\n",
    "    subheadings = specialty_section.find_all('h3')\n",
    "\n",
    "    for subheading in subheadings:\n",
    "        subheading_text = subheading.get_text(strip=True)  # Get specialty name\n",
    "        links = []\n",
    "\n",
    "        # Find the next unordered list (ul) which contains links\n",
    "        next_ul = subheading.find_next_sibling('ul')\n",
    "\n",
    "        if next_ul:\n",
    "            for a_tag in next_ul.find_all('a', href=True):\n",
    "                link_text = a_tag.get_text(strip=True)  # Link display name\n",
    "                link_href = a_tag['href']  # Actual URL\n",
    "                links.append({'display_name': link_text, 'url': link_href})\n",
    "\n",
    "        results.append({'specialty': subheading_text, 'links': links})\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_likelihood_ratios(page_content):\n",
    "    \"\"\"\n",
    "    Parses all likelihood ratio tables within <article class=\"lr_cards_details\">.\n",
    "    For each subsection indicated by an <h3> heading:\n",
    "      - If the heading indicates Positive Findings, each finding will be prefixed with \"Patient has: \".\n",
    "      - If the heading indicates Negative Findings, a leading \"No\" (if present) is removed from the finding and then it is prefixed with \"Patient does not have: \".\n",
    "    This function processes all tables under a given heading (i.e. until the next <h3> is reached).\n",
    "    Returns a list of tuples: (finding, likelihood ratio).\n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    results = []\n",
    "    \n",
    "    # Locate the main LR details section.\n",
    "    lr_section = soup.find('article', class_='lr_cards_details')\n",
    "    if not lr_section:\n",
    "        return results\n",
    "\n",
    "    # Find all <h3> headings in the section.\n",
    "    headings = lr_section.find_all('h3')\n",
    "    \n",
    "    if headings:\n",
    "        for h3 in headings:\n",
    "            heading_text = h3.get_text(strip=True)\n",
    "            if \"Positive Findings\" in heading_text:\n",
    "                prefix = \"Patient has: \"\n",
    "            elif \"Negative Findings\" in heading_text:\n",
    "                prefix = \"Patient does not have: \"\n",
    "            else:\n",
    "                prefix = \"\"\n",
    "            \n",
    "            # Process all sibling elements until the next <h3> is encountered.\n",
    "            sibling = h3.find_next_sibling()\n",
    "            while sibling and sibling.name != \"h3\":\n",
    "                if sibling.name == \"table\" and \"lrtable\" in sibling.get(\"class\", []):\n",
    "                    # Try to get proper data rows (i.e. <tr> elements with <td>).\n",
    "                    rows = sibling.find_all(\"tr\")\n",
    "                    data_rows = [row for row in rows if row.find_all(\"td\")]\n",
    "                    \n",
    "                    if data_rows:\n",
    "                        for row in data_rows:\n",
    "                            cols = row.find_all(\"td\")\n",
    "                            if len(cols) >= 2:\n",
    "                                finding = cols[0].get_text(strip=True)\n",
    "                                lr_value = cols[1].get_text(strip=True)\n",
    "                                # If there's an <a> inside the LR cell, use its text.\n",
    "                                link = cols[1].find(\"a\")\n",
    "                                if link:\n",
    "                                    lr_value = link.get_text(strip=True) or lr_value\n",
    "                                if not lr_value:\n",
    "                                    lr_value = \"Not reported\"\n",
    "                                \n",
    "                                # Modify the finding string based on the prefix.\n",
    "                                if prefix:\n",
    "                                    if prefix.startswith(\"Patient does not have:\"):\n",
    "                                        finding = re.sub(r'^no\\s+', '', finding, flags=re.IGNORECASE)\n",
    "                                    finding = prefix + finding\n",
    "                                \n",
    "                                results.append((finding, lr_value))\n",
    "                    else:\n",
    "                        # If no rows with <td> are found, assume the table contains <td> elements in sequence.\n",
    "                        all_tds = sibling.find_all(\"td\")\n",
    "                        # Process in pairs.\n",
    "                        for i in range(0, len(all_tds), 2):\n",
    "                            finding = all_tds[i].get_text(strip=True)\n",
    "                            if i+1 < len(all_tds):\n",
    "                                lr_value = all_tds[i+1].get_text(strip=True)\n",
    "                            else:\n",
    "                                lr_value = \"Not reported\"\n",
    "                            # Check for an <a> element.\n",
    "                            a_tag = all_tds[i+1].find(\"a\")\n",
    "                            if a_tag:\n",
    "                                lr_value = a_tag.get_text(strip=True) or lr_value\n",
    "                            if not lr_value:\n",
    "                                lr_value = \"Not reported\"\n",
    "                            \n",
    "                            if prefix:\n",
    "                                if prefix.startswith(\"Patient does not have:\"):\n",
    "                                    finding = re.sub(r'^no\\s+', '', finding, flags=re.IGNORECASE)\n",
    "                                finding = prefix + finding\n",
    "                            results.append((finding, lr_value))\n",
    "                sibling = sibling.find_next_sibling()\n",
    "    else:\n",
    "        # Fallback: process all tables in the section if no <h3> headings exist.\n",
    "        tables = lr_section.find_all('table', class_='lrtable')\n",
    "        for table in tables:\n",
    "            rows = table.find_all(\"tr\")\n",
    "            data_rows = [row for row in rows if row.find_all(\"td\")]\n",
    "            if data_rows:\n",
    "                for row in data_rows:\n",
    "                    cols = row.find_all(\"td\")\n",
    "                    if len(cols) >= 2:\n",
    "                        finding = cols[0].get_text(strip=True)\n",
    "                        lr_value = cols[1].get_text(strip=True)\n",
    "                        link = cols[1].find(\"a\")\n",
    "                        if link:\n",
    "                            lr_value = link.get_text(strip=True) or lr_value\n",
    "                        if not lr_value:\n",
    "                            lr_value = \"Not reported\"\n",
    "                        results.append((finding, lr_value))\n",
    "            else:\n",
    "                all_tds = table.find_all(\"td\")\n",
    "                for i in range(0, len(all_tds), 2):\n",
    "                    finding = all_tds[i].get_text(strip=True)\n",
    "                    if i+1 < len(all_tds):\n",
    "                        lr_value = all_tds[i+1].get_text(strip=True)\n",
    "                    else:\n",
    "                        lr_value = \"Not reported\"\n",
    "                    a_tag = all_tds[i+1].find(\"a\") if i+1 < len(all_tds) else None\n",
    "                    if a_tag:\n",
    "                        lr_value = a_tag.get_text(strip=True) or lr_value\n",
    "                    if not lr_value:\n",
    "                        lr_value = \"Not reported\"\n",
    "                    results.append((finding, lr_value))\n",
    "                    \n",
    "    return results\n",
    "\n",
    "def fetch_webpages(specialty_links):\n",
    "    \"\"\"\n",
    "    Iterates through all the extracted links, fetches the webpage content, \n",
    "    and extracts likelihood ratio findings.\n",
    "    \"\"\"\n",
    "    findings_by_display_name = {}\n",
    "\n",
    "    for item in specialty_links:\n",
    "        print(f\"Fetching pages for Specialty: {item['specialty']}\")\n",
    "\n",
    "        for link in item['links']:\n",
    "            display_name = link['display_name']\n",
    "            url = link['url']\n",
    "\n",
    "            try:\n",
    "                print(f\"  - Fetching: {display_name} ({url})\")\n",
    "                response = requests.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"    Success: {display_name} page fetched.\")\n",
    "                    \n",
    "                    # Extract likelihood ratio findings\n",
    "                    findings = extract_likelihood_ratios(response.text)\n",
    "                    \n",
    "                    # Store the extracted data\n",
    "                    findings_by_display_name[display_name] = findings\n",
    "\n",
    "                else:\n",
    "                    print(f\"    Failed to fetch {display_name} - Status Code: {response.status_code}\")\n",
    "\n",
    "                time.sleep(1)  # Optional: Add a delay to avoid overwhelming the server\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"    Error fetching {display_name}: {e}\")\n",
    "\n",
    "        print(\"\\n\")  # Add space between specialties for readability\n",
    "\n",
    "    return findings_by_display_name\n",
    "\n",
    "def save_to_excel(findings_data, filename=\"nnt_lrs.xlsx\", blank_values=False):\n",
    "    \"\"\"\n",
    "    Saves likelihood ratios to an Excel file with each display_name as a separate sheet.\n",
    "    If blank_values is True, the Likelihood Ratio column is left blank.\n",
    "    The first row contains the full display_name, and column headers start from the second row.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
    "        for display_name, findings in findings_data.items():\n",
    "            if findings:\n",
    "                # Prepare DataFrame\n",
    "                df = pd.DataFrame(findings, columns=[\"Finding\", \"Likelihood Ratio\"])\n",
    "\n",
    "                if blank_values:\n",
    "                    df[\"Likelihood Ratio\"] = \"\"  # Clear likelihood ratio values\n",
    "\n",
    "                # Insert full display_name as the first row\n",
    "                full_name_row = pd.DataFrame({df.columns[0]: [display_name], df.columns[1]: [\"\"]})\n",
    "                df = pd.concat([full_name_row, df], ignore_index=True)\n",
    "\n",
    "                # Save to Excel with sheet name as the **last** 31 characters\n",
    "                sheet_name = display_name[-31:]\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)  # No default header\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping {display_name} (No data found).\")\n",
    "\n",
    "    print(f\"\\nLikelihood ratios saved to {filename}\")\n",
    "\n",
    "# Fetch specialties and links\n",
    "specialty_links = get_specialty_links()\n",
    "findings_data = fetch_webpages(specialty_links)\n",
    "\n",
    "# Save normal file\n",
    "save_to_excel(findings_data, \"nnt_lrs.xlsx\", blank_values=False)\n",
    "\n",
    "# Save version with blank likelihood ratios\n",
    "save_to_excel(findings_data, \"nnt_lrs_sans_number.xlsx\", blank_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert LR's to numerical format\n",
    "\n",
    "This block takes in the excel spreadsheet with raw data from theNNT.com (\"nnt_lrs.xlsx\") and creates a new spreadsheet (\"nnt_lrs_processed.xlsx\") with a third column that contains a numerical version of the second column (raw data from theNNT) to be used as the LR_llm.\n",
    "\n",
    "It removes any 'x's from the input, then determines whether the cell reports \n",
    "\n",
    "1. point estimate only (in which case use the point estimate)\n",
    "2. point estimate + range (in which case take the point estimate), or \n",
    "3. range only (in which case, calculate the geometric mean)\n",
    "\n",
    "It also counts the number of conditions (last 31 letters due to excel limitation) and LRs\n",
    "\n",
    "[ ] TODO / Issues list: \n",
    "- some of the BNP thresholds just have a number rather than a specification of the full \"BNP > 100\"; need to use header e.g. https://thennt.com/lr/dyspnea-due-to-heart-failure-without-chronic-respiratory-disease/ and https://thennt.com/lr/dyspnea-due-to-heart-failure-without-chronic-respiratory-disease/ - for the manuscript, these had to be manually reconciled.\n",
    "- some of the tables on the NNT contain categories that define the type of finding (ie 'edema' in a 'Chest X-ray findings') - these are manually reconciled/cleaned up for the manuscript as well. \n",
    "- some of the \"Prediction Targets\" contain specifications not relevant to the task (ie. page title = \"Accuracy of Ultrasound Findings for Diagnosing Retinal Pathology..\" but prediction target is Retinal Pathology). This is also manually cleaned for the manuscript.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lr(lr_str):\n",
    "    \"\"\"\n",
    "    Given a string from the 'Likelihood Ratio' cell, this function:\n",
    "      - Removes any 'x' characters from the input.\n",
    "      - If the string contains a parenthesized range (i.e. a point estimate plus a range),\n",
    "        it returns the point estimate.\n",
    "      - If the entire string is a range (e.g. \"0.92-1.1\", \"3.3 to 4.8\", \"4.8–7.6\"),\n",
    "        it computes and returns the geometric mean.\n",
    "      - Otherwise, it returns a float based on the first number found.\n",
    "      - If the value is missing or cannot be parsed, returns NaN.\n",
    "    \"\"\"\n",
    "    # Remove all 'x' characters and trim whitespace\n",
    "    lr_str = lr_str.replace(\"x\", \"\").strip()\n",
    "    if lr_str == \"\":\n",
    "        return np.nan\n",
    "\n",
    "    # If parentheses exist, assume format \"point_estimate (range)\" and use the point estimate.\n",
    "    if \"(\" in lr_str:\n",
    "        point_part = lr_str.split(\"(\")[0].strip()\n",
    "        try:\n",
    "            return float(point_part)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Check for a range-only pattern.\n",
    "    # This regex looks for two numbers separated by \"to\", \"-\" or \"–\" with optional whitespace.\n",
    "    range_only_match = re.match(r'^\\s*([0-9]*\\.?[0-9]+)\\s*(to|[-–])\\s*([0-9]*\\.?[0-9]+)\\s*$', lr_str)\n",
    "    if range_only_match:\n",
    "        try:\n",
    "            low = float(range_only_match.group(1))\n",
    "            high = float(range_only_match.group(3))\n",
    "            return math.sqrt(low * high)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    # Fallback: if no range-only pattern is found, extract the first number and return it.\n",
    "    numbers = re.findall(r'([0-9]*\\.?[0-9]+)', lr_str)\n",
    "    if numbers:\n",
    "        try:\n",
    "            return float(numbers[0])\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Load the original Excel file (each sheet has no header and the first row is the display name row)\n",
    "input_filename = \"nnt_lrs.xlsx\"\n",
    "output_filename = \"nnt_lrs_processed.xlsx\"\n",
    "\n",
    "# Read all sheets from the Excel file into a dictionary of DataFrames.\n",
    "excel_sheets = pd.read_excel(input_filename, sheet_name=None, header=None)\n",
    "\n",
    "total_lr_count = 0\n",
    "sheet_counts = {}\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine=\"openpyxl\") as writer:\n",
    "    for sheet_name, df in excel_sheets.items():\n",
    "        numerical_lr = []\n",
    "        # Process each row in the sheet.\n",
    "        for idx, row in df.iterrows():\n",
    "            # For the header row (assumed to be the first row: condition label), add an empty string.\n",
    "            if idx == 0:\n",
    "                numerical_lr.append(\"\")\n",
    "            else:\n",
    "                cell_value = row[1]  # The original \"Likelihood Ratio\" is in the second column (index 1)\n",
    "                if pd.isna(cell_value) or str(cell_value).strip() == \"\":\n",
    "                    numerical_lr.append(\"\")\n",
    "                else:\n",
    "                    numerical_lr.append(parse_lr(str(cell_value)))\n",
    "        \n",
    "        # Insert the new column immediately after the \"Likelihood Ratio\" column.\n",
    "        # This makes the new column the third column.\n",
    "        df.insert(2, \"Numerical LR\", numerical_lr)\n",
    "        \n",
    "        # Remove rows (except the header) where the new \"Numerical LR\" is empty or NaN.\n",
    "        header = df.iloc[[0]]  # Keep the header row (the condition label)\n",
    "        data = df.iloc[1:]\n",
    "        data = data[data[\"Numerical LR\"].apply(lambda x: not (x == \"\" or pd.isna(x)))]\n",
    "        filtered_df = pd.concat([header, data], ignore_index=True)\n",
    "        \n",
    "        # Insert a new row (after the condition label row) with the column labels.\n",
    "        # The final sheet will have:\n",
    "        #   Row 0: Condition label (from the original sheet)\n",
    "        #   Row 1: Column labels: 'finding', 'lr_raw', and 'lr_num'\n",
    "        #   Row 2+: Data rows\n",
    "        col_labels = pd.DataFrame([[\"finding\", \"lr_raw\", \"lr_reported\"]], columns=filtered_df.columns)\n",
    "        final_df = pd.concat([filtered_df.iloc[[0]], col_labels, filtered_df.iloc[1:]], ignore_index=True)\n",
    "        \n",
    "        # Count the number of LR values for this sheet (exclude the two header rows).\n",
    "        lr_count = len(final_df) - 2\n",
    "        sheet_counts[sheet_name] = lr_count\n",
    "        total_lr_count += lr_count\n",
    "        \n",
    "        # Write the modified DataFrame to the new Excel file.\n",
    "        # The output maintains the original format: no index and no additional header row.\n",
    "        final_df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "# Display counts.\n",
    "num_sheets = len(excel_sheets)\n",
    "print(f\"Processed {num_sheets} condition(s) (sheets).\")\n",
    "for sheet, count in sheet_counts.items():\n",
    "    print(f\"Sheet '{sheet}' has {count} LR value(s).\")\n",
    "print(f\"Total LR values processed across all sheets: {total_lr_count}.\")\n",
    "\n",
    "print(f\"Processed Excel file saved as '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate LRs\n",
    "\n",
    "This code block reads in the data from the nnt_lr_processed.xlsx excel file and calls a list of openAI models to have them give there best (single) estimate of the LR. Then, it rights a new spreadsheet nnt_lr_estimates that includes columns in each spreadsheet for each estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newest Version (Aug 22nd)\n",
    "\n",
    "GPT-5 Capabilities added and revised prompting strategy mildly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Confirmation of Endotracheal Tube Placement | gpt-5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bayesian LR estimator — Responses API\n",
    "Updated: 2025‑08‑22\n",
    "\n",
    "Changes vs. your prior block:\n",
    "• Responses API only (no Chat fallback).\n",
    "• No max_output_tokens (output is schema‑constrained and tiny).\n",
    "• Reasoning models use reasoning.effort=\"medium\"; no temperature/top_p.\n",
    "• GPT‑5(+mini) use text.verbosity=\"low\" (applied only where supported).\n",
    "• Includes these models: gpt‑5, gpt‑5‑mini, gpt‑4o‑mini‑2024‑07‑18, gpt‑4o‑2024‑08‑06,\n",
    "  gpt‑4.1‑mini‑2025‑04‑14, gpt‑4.1‑2025‑04‑14, o3‑mini‑2025‑01‑31, o3‑2025‑04‑16, o4‑mini‑2025‑04‑16.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import time, math\n",
    "from random import random\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Model registry: \n",
    "MODEL_CAPABILITIES = {\n",
    "    # GPT‑5 series (reasoning; supports text.verbosity; no temperature)\n",
    "    \"gpt-5\"        : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "    \"gpt-5-mini\"   : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "    \"gpt-5-nano\"   : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "\n",
    "    # GPT‑4.1 family (non‑reasoning; temperature OK); include snapshots + aliases\n",
    "    \"gpt-4.1-2025-04-14\" : {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4.1-mini-2025-04-14\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4.1-nano-2025-04-14\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "\n",
    "    # GPT‑4o family (non‑reasoning; temperature OK); prefer latest snapshot or alias\n",
    "    \"gpt-4o-2024-11-20\"  : {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4o-mini-2024-07-18\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "\n",
    "    # o‑series (reasoning; no temperature)\n",
    "    \"o3-2025-04-16\" : {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "    \"o3-mini-2025-01-31\": {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "    \"o4-mini-2025-04-16\": {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "}\n",
    "MODELS = list(MODEL_CAPABILITIES)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "INPUT_FILE  = \"nnt_lrs_processed.xlsx\"\n",
    "OUTPUT_FILE = \"nnt_lrs_with_estimated.xlsx\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Prompt \n",
    "# -----------------------------------------------------------------------------\n",
    "SYSTEM_CORE = \"\"\"You are a Bayesian diagnostic assistant.\n",
    "Estimate a numeric likelihood ratio (LR) for a finding with respect to a diagnosis.\n",
    "Return only a JSON object matching the schema: {\"value\": <float>}, where value > 0.\n",
    "\"\"\"\n",
    "\n",
    "DEFINITION = \"\"\"Definition:\n",
    "LR = P(finding | diagnosis) / P(finding | not-diagnosis)\n",
    "\"\"\"\n",
    "\n",
    "BANDS = \"\"\"LR evidence bands (reference):\n",
    ">10 strong for; 5-10 moderate for; 2–5 weak for;\n",
    "0.5–2 negligible;\n",
    "0.2-0.5 weak against; 0.1-0.2 moderate against; ≤0.1 strong against\"\"\"\n",
    "\n",
    "# Few‑shot examples - these are human guestimates (to avoid seeding the dataset and inflating performance)\n",
    "FEW_SHOT_RICH = [\n",
    "    (\"deep vein thrombosis\",    \"femoral vein noncompressaible on ultrasound\",      16.0), # some data this might be higher? \n",
    "    (\"pericarditis\",            \"pleuritic chest pain improved by leaning forward\",  5.2),\n",
    "    (\"pulmonary embolism\",      \"tachycardia >100 bpm\",                              2.2),\n",
    "    (\"urinary tract infection\", \"malodorous urine\",                                  1.1),\n",
    "    (\"myocardial infarction\",  \"enjoys playing chess\",                               1.0),\n",
    "    (\"appendicitis\",            \"no RLQ tenderness\",                                0.45),\n",
    "    (\"pneumothorax\",            \"bilateral lung sliding present on US\",             0.18), # some data this might be lower?\n",
    "    (\"HIV infection\",           \"4th‑generation Ag/Ab screen negative beyond window\",0.05),\n",
    "\n",
    "]\n",
    "\n",
    "FEW_SHOT_MIN = [\n",
    "    (\"deep vein thrombosis\",    \"femoral vein noncompressaible on ultrasound\",     16.0),\n",
    "    (\"myocardial infarction\",  \"enjoys playing chess\",                             1.0),\n",
    "]\n",
    "\n",
    "def build_messages(diagnosis: str, finding: str, reasoning: bool) -> list[dict]:\n",
    "    msgs: list[dict] = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_CORE.strip()},\n",
    "        {\"role\": \"system\", \"content\": DEFINITION.strip()},\n",
    "        {\"role\": \"system\", \"content\": BANDS.strip()},\n",
    "    ]\n",
    "    examples = FEW_SHOT_MIN if reasoning else FEW_SHOT_RICH\n",
    "    for dx_ex, f_ex, v_ex in examples:\n",
    "        msgs.append({\"role\": \"user\",      \"content\": f\"Condition: {dx_ex}\\nFinding: {f_ex}\"})\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": f'{{\"value\": {float(v_ex)}}}'})\n",
    "    msgs.append({\"role\": \"user\", \"content\": f\"Condition: {diagnosis}\\nFinding: {finding}\"})\n",
    "    return msgs\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Structured Outputs schema (Pydantic)\n",
    "# -----------------------------------------------------------------------------\n",
    "class LRResponse(BaseModel):\n",
    "    value: float\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2b) Retry wrapper (exponential backoff with jitter)\n",
    "# -----------------------------------------------------------------------------\n",
    "def estimate_lr_until_positive(\n",
    "    diagnosis: str,\n",
    "    finding: str,\n",
    "    model: str,\n",
    "    client: Optional[OpenAI] = None,\n",
    "    max_retries: Optional[int] = None,      # None ⇒ retry indefinitely\n",
    "    base_backoff: float = 0.5,              # seconds\n",
    "    max_backoff: float = 30.0               # seconds\n",
    ") -> float:\n",
    "    attempt = 0\n",
    "    while True:\n",
    "        attempt += 1\n",
    "        try:\n",
    "            lr = estimate_lr(diagnosis, finding, model, client)\n",
    "            if isinstance(lr, (int, float)) and math.isfinite(lr) and lr > 0:\n",
    "                return float(lr)\n",
    "            raise ValueError(f\"Non‑positive or non‑finite LR: {lr!r}\")\n",
    "        except Exception as e:\n",
    "            logging.warning(\n",
    "                f\"[retry {attempt}] sheet finding='{finding[:80]}' | \"\n",
    "                f\"model={model} → {e}\"\n",
    "            )\n",
    "            if (max_retries is not None) and (attempt >= max_retries):\n",
    "                raise\n",
    "            # exponential backoff with jitter\n",
    "            delay = min(base_backoff * (2 ** (attempt - 1)), max_backoff)\n",
    "            time.sleep(delay * (0.5 + random()))  # 0.5–1.5× jitter\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) Estimator call (Responses API)\n",
    "# -----------------------------------------------------------------------------\n",
    "def estimate_lr(diagnosis: str, finding: str, model: str, client: Optional[OpenAI] = None) -> float:\n",
    "    if client is None:\n",
    "        client = OpenAI()\n",
    "\n",
    "    cfg = MODEL_CAPABILITIES[model]\n",
    "    msgs = build_messages(diagnosis, finding, reasoning=cfg[\"reasoning\"])\n",
    "\n",
    "    kwargs = {}\n",
    "    if cfg[\"reasoning\"]:\n",
    "        kwargs[\"reasoning\"] = {\"effort\": \"medium\"}     # for GPT‑5 and o‑series\n",
    "        # no temperature/top_p\n",
    "    elif cfg[\"allow_temp\"]:\n",
    "        kwargs[\"temperature\"] = 0.2                    # allowed for 4o / 4.1\n",
    "\n",
    "    # Apply verbosity only where supported (GPT‑5 family)\n",
    "    if cfg[\"verbosity\"]:\n",
    "        kwargs[\"text\"] = {\"verbosity\": \"low\"}\n",
    "\n",
    "    resp = client.responses.parse(\n",
    "        model=model,\n",
    "        input=msgs,\n",
    "        text_format=LRResponse,    # Structured Outputs → Pydantic\n",
    "        **kwargs,\n",
    "    )\n",
    "    return float(resp.output_parsed.value)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Main pipeline: read workbook → append model columns → write output\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_batch(input_file: str | Path, output_file: str | Path, models: list[str]) -> None:\n",
    "    sheets = pd.read_excel(input_file, sheet_name=None, header=None)\n",
    "\n",
    "    for sheet_name, df in sheets.items():\n",
    "        diagnosis = str(df.iloc[0, 0]).strip()\n",
    "        for model in models:\n",
    "            new_header = \"lr_\" + model\n",
    "            col = []\n",
    "            print(f\"→ {diagnosis[:60]} | {model}\")\n",
    "            for i in range(len(df)):\n",
    "                if i == 0:\n",
    "                    col.append(\"\")                   # top-left cell (sheet label row)\n",
    "                elif i == 1:\n",
    "                    col.append(new_header)           # column header row\n",
    "                else:\n",
    "                    finding = str(df.iloc[i, 0]).strip()\n",
    "                    if not finding:\n",
    "                        col.append(\"\")               # keep blank rows blank\n",
    "                        continue\n",
    "                    try:\n",
    "                        # retry until a strictly positive, finite float is returned\n",
    "                        lr = estimate_lr_until_positive(\n",
    "                            diagnosis, finding, model, client,\n",
    "                            max_retries=None         # set to an int (e.g., 8) to cap retries\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        lr = \"ERROR\"\n",
    "                        logging.warning(\n",
    "                            f\"Error on sheet '{sheet_name}', row {i}, model {model} after retries: {e}\"\n",
    "                        )\n",
    "                    col.append(lr)\n",
    "            # Insert as object dtype to accommodate strings like \"ERROR\"\n",
    "            df.insert(df.shape[1], new_header, pd.Series(col, dtype=\"object\"))\n",
    "        sheets[sheet_name] = df\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        for name, frame in sheets.items():\n",
    "            frame.to_excel(writer, sheet_name=name, index=False, header=False)\n",
    "\n",
    "    print(f\"Done – results saved to '{output_file}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_batch(INPUT_FILE, OUTPUT_FILE, MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collate the outputs into a single sheet \n",
    "\n",
    "Note: the automated classify_feature_type performance is low, thus the manuscript relies on manual classification of the feature type. \n",
    "\n",
    "In the call for the convert_lr_workbook, 3 files must be specified\n",
    "\n",
    "- input_file = the output file from the above LR estimator loop. \n",
    "- output_file = where to write the output\n",
    "- template_file = an example workbook that contains the desired output format (for specification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1.  ***Optional*** helper – keep the same “Feature Type” buckets   \n",
    "#  NOTE: this does not seem like it works very well - just using manual classification for the manuscript\n",
    "# ------------------------------------------------------------------ #\n",
    "def classify_feature_type(text: str) -> str:\n",
    "    \"\"\"Heuristic that matches the legacy categories.\"\"\"\n",
    "    t = str(text).lower()\n",
    "\n",
    "    history = \"history:\" in t\n",
    "    sign    = (\"sign:\" in t) or (\"symptom\" in t)\n",
    "    score   = any(k in t for k in (\"score\", \"points\", \"rule\"))\n",
    "    test    = any(k in t for k in (\"test:\", \"lab\", \"troponin\", \"d‑dimer\"))\n",
    "    img     = any(k in t for k in (\n",
    "        \"ultrasound\", \"ct\", \"mri\", \"x‑ray\", \"radiograph\", \"imaging\",\n",
    "        \"echo\", \"angiogram\"))\n",
    "\n",
    "    if history and test: return \"History and Test\"\n",
    "    if history and img:  return \"History and imaging\"\n",
    "    if history:          return \"History_\"\n",
    "    if sign:             return \"Sign_symptom\"\n",
    "    if score:            return \"Score\"\n",
    "    if test:             return \"Test finding\"\n",
    "    if img:              return \"Imaging finding\"\n",
    "    return \"Diagnosis\"\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2.  Converter – writes **one** sheet (same name as in template)     #\n",
    "# ------------------------------------------------------------------ #\n",
    "def convert_lr_workbook(\n",
    "    *,                       # keyword‑only for clarity\n",
    "    input_file: str | Path,  # e.g. \"nnt_lrs_with_estimated.xlsx\"\n",
    "    template_file: str | Path,  # updated example workbook\n",
    "    output_file: str | Path = \"nnt_lrs_converted.xlsx\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    • Collapses all per‑condition tabs from `input_file` into a master frame.\n",
    "    • Adds the right‑most `condition` column (original tab name).\n",
    "    • Writes a single worksheet whose name matches the (only) sheet\n",
    "      found in `template_file`, with the header row stored as **row 1**\n",
    "      (no Excel column headers) to preserve downstream‑notebook compatibility.\n",
    "    \"\"\"\n",
    "    input_file    = Path(input_file)\n",
    "    template_file = Path(template_file)\n",
    "    output_file   = Path(output_file)\n",
    "\n",
    "    # -------- determine the sole sheet name from the template --------------\n",
    "    template_xls = pd.ExcelFile(template_file, engine=\"openpyxl\")\n",
    "    if len(template_xls.sheet_names) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Template has {len(template_xls.sheet_names)} sheets; \"\n",
    "            \"expected exactly one after pruning.\"\n",
    "        )\n",
    "    target_sheet = template_xls.sheet_names[0]  # e.g. \"Master\"\n",
    "\n",
    "    # ------------------- build master dataframe ----------------------------\n",
    "    in_xls = pd.ExcelFile(input_file, engine=\"openpyxl\")\n",
    "    frames = []\n",
    "\n",
    "    for tab in in_xls.sheet_names:\n",
    "        raw = pd.read_excel(in_xls, sheet_name=tab, header=None)\n",
    "\n",
    "        # Expect: row‑0 = diagnosis, row‑1 = column labels, row‑2+ = data\n",
    "        if raw.shape[0] < 3:\n",
    "            continue  # skip empty or malformed tabs\n",
    "\n",
    "        header = raw.iloc[1]\n",
    "        df = raw.iloc[2:].copy()\n",
    "        df.columns = header\n",
    "        df = df.loc[:, ~df.columns.isna()]  # drop unnamed columns\n",
    "\n",
    "        # back‑fill Feature Type if the sheet didn't have it\n",
    "        if \"Feature Type\" not in df.columns:\n",
    "            df[\"Feature Type\"] = df.iloc[:, 0].apply(classify_feature_type)\n",
    "\n",
    "        df[\"condition\"] = tab          # new right‑most column\n",
    "        frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(\"No valid data found in the input workbook.\")\n",
    "\n",
    "    master = pd.concat(frames, ignore_index=True)\n",
    "    # ensure 'condition' is the final column\n",
    "    master = master[[c for c in master.columns if c != \"condition\"] + [\"condition\"]]\n",
    "\n",
    "    # ------------------------ write single sheet ---------------------------\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        # replicate legacy layout: header row lives **inside** the block\n",
    "        block = pd.concat(\n",
    "            [pd.DataFrame([master.columns], columns=master.columns), master],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        block.to_excel(\n",
    "            writer,\n",
    "            sheet_name=target_sheet[:31],  # Excel has a 31‑char cap\n",
    "            index=False,\n",
    "            header=False,\n",
    "        )\n",
    "\n",
    "    print(f\"Converted workbook written to: {output_file} \"\n",
    "          f\"(single sheet: '{target_sheet}')\")\n",
    "    \n",
    "# Run the conversion\n",
    "convert_lr_workbook(\n",
    "    input_file   = 'nnt_lrs_with_estimated.xlsx',   # produced after LR loop\n",
    "    template_file= 'example_template.xlsx',  # updated example\n",
    "    output_file  = 'NNT_LRs_08-26-2025.xlsx',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
