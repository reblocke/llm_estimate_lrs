{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Scraper and Estimator\n",
    "\n",
    "Overall, this notebook contains code to scrape diagnostic likelihood ratios from theNNT.com and convert them to numerical form. \n",
    "\n",
    "It also contains code to generate prompts for large language models to estimate the likelihood ratios. \n",
    "\n",
    "The output is a spreadsheet called: nnt_lrs_with_estimated which contains: \n",
    "- a sheet for each diagnosis or prediction target\n",
    "- a row for each piece of information\n",
    "- columns for the name, raw nnt lr, processed nnt lr, and estimated by 1 or more LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Data from the NNT \n",
    "\n",
    "This scrapes all of the likelihood ratios listed on the NNT ('https://thennt.com/home-lr/') into Excel spreadsheets. \n",
    "\n",
    "1. A spreadsheet (\"nnt_lrs.xlsx\") contains a separate sheet for each page, which corresponds to a \"prediction tasks\" e.g. diagnosing the cause of a symptom - sometimes with specification of an intended population. Each sheet contains two columns: the name of the features (e.g. test result, finding, historical occurence, comorbditiy), the second contains the raw listing from the spreadsheet\n",
    "\n",
    "2. A second spreadsheet contains the same sheets corresponding to a prediction target, and all of the features. These are used two call the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI  # or your appropriate client wrapper\n",
    "import math\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # looks for a .env file in the current dir by default\n",
    "#print(os.getenv(\"OPENAI_API_KEY\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_specialty_links():\n",
    "    \"\"\"\n",
    "    Extracts specialties and their corresponding article links from the webpage.\n",
    "    Returns a list of dictionaries with specialty names and associated links.\n",
    "    \"\"\"\n",
    "\n",
    "    url = 'https://thennt.com/home-lr/'\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Locate the section with \"Diagnosis (LR) Reviews by Specialty\"\n",
    "    specialty_section = soup.find('div', class_='well subdisplay accordion_caption', id='lr-byspecialty')\n",
    "\n",
    "    if not specialty_section:\n",
    "        print(\"Could not find the 'Diagnosis (LR) Reviews by Specialty' section on the webpage.\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Find all specialty headings (e.g., h3)\n",
    "    subheadings = specialty_section.find_all('h3')\n",
    "\n",
    "    for subheading in subheadings:\n",
    "        subheading_text = subheading.get_text(strip=True)  # Get specialty name\n",
    "        links = []\n",
    "\n",
    "        # Find the next unordered list (ul) which contains links\n",
    "        next_ul = subheading.find_next_sibling('ul')\n",
    "\n",
    "        if next_ul:\n",
    "            for a_tag in next_ul.find_all('a', href=True):\n",
    "                link_text = a_tag.get_text(strip=True)  # Link display name\n",
    "                link_href = a_tag['href']  # Actual URL\n",
    "                links.append({'display_name': link_text, 'url': link_href})\n",
    "\n",
    "        results.append({'specialty': subheading_text, 'links': links})\n",
    "\n",
    "    return results\n",
    "\n",
    "def extract_likelihood_ratios(page_content):\n",
    "    \"\"\"\n",
    "    Parses all likelihood ratio tables within <article class=\"lr_cards_details\">.\n",
    "    For each subsection indicated by an <h3> heading:\n",
    "      - If the heading indicates Positive Findings, each finding will be prefixed with \"Patient has: \".\n",
    "      - If the heading indicates Negative Findings, a leading \"No\" (if present) is removed from the finding and then it is prefixed with \"Patient does not have: \".\n",
    "    This function processes all tables under a given heading (i.e. until the next <h3> is reached).\n",
    "    Returns a list of tuples: (finding, likelihood ratio).\n",
    "    \"\"\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import re\n",
    "\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    results = []\n",
    "    \n",
    "    # Locate the main LR details section.\n",
    "    lr_section = soup.find('article', class_='lr_cards_details')\n",
    "    if not lr_section:\n",
    "        return results\n",
    "\n",
    "    # Find all <h3> headings in the section.\n",
    "    headings = lr_section.find_all('h3')\n",
    "    \n",
    "    if headings:\n",
    "        for h3 in headings:\n",
    "            heading_text = h3.get_text(strip=True)\n",
    "            if \"Positive Findings\" in heading_text:\n",
    "                prefix = \"Patient has: \"\n",
    "            elif \"Negative Findings\" in heading_text:\n",
    "                prefix = \"Patient does not have: \"\n",
    "            else:\n",
    "                prefix = \"\"\n",
    "            \n",
    "            # Process all sibling elements until the next <h3> is encountered.\n",
    "            sibling = h3.find_next_sibling()\n",
    "            while sibling and sibling.name != \"h3\":\n",
    "                if sibling.name == \"table\" and \"lrtable\" in sibling.get(\"class\", []):\n",
    "                    # Try to get proper data rows (i.e. <tr> elements with <td>).\n",
    "                    rows = sibling.find_all(\"tr\")\n",
    "                    data_rows = [row for row in rows if row.find_all(\"td\")]\n",
    "                    \n",
    "                    if data_rows:\n",
    "                        for row in data_rows:\n",
    "                            cols = row.find_all(\"td\")\n",
    "                            if len(cols) >= 2:\n",
    "                                finding = cols[0].get_text(strip=True)\n",
    "                                lr_value = cols[1].get_text(strip=True)\n",
    "                                # If there's an <a> inside the LR cell, use its text.\n",
    "                                link = cols[1].find(\"a\")\n",
    "                                if link:\n",
    "                                    lr_value = link.get_text(strip=True) or lr_value\n",
    "                                if not lr_value:\n",
    "                                    lr_value = \"Not reported\"\n",
    "                                \n",
    "                                # Modify the finding string based on the prefix.\n",
    "                                if prefix:\n",
    "                                    if prefix.startswith(\"Patient does not have:\"):\n",
    "                                        finding = re.sub(r'^no\\s+', '', finding, flags=re.IGNORECASE)\n",
    "                                    finding = prefix + finding\n",
    "                                \n",
    "                                results.append((finding, lr_value))\n",
    "                    else:\n",
    "                        # If no rows with <td> are found, assume the table contains <td> elements in sequence.\n",
    "                        all_tds = sibling.find_all(\"td\")\n",
    "                        # Process in pairs.\n",
    "                        for i in range(0, len(all_tds), 2):\n",
    "                            finding = all_tds[i].get_text(strip=True)\n",
    "                            if i+1 < len(all_tds):\n",
    "                                lr_value = all_tds[i+1].get_text(strip=True)\n",
    "                            else:\n",
    "                                lr_value = \"Not reported\"\n",
    "                            # Check for an <a> element.\n",
    "                            a_tag = all_tds[i+1].find(\"a\")\n",
    "                            if a_tag:\n",
    "                                lr_value = a_tag.get_text(strip=True) or lr_value\n",
    "                            if not lr_value:\n",
    "                                lr_value = \"Not reported\"\n",
    "                            \n",
    "                            if prefix:\n",
    "                                if prefix.startswith(\"Patient does not have:\"):\n",
    "                                    finding = re.sub(r'^no\\s+', '', finding, flags=re.IGNORECASE)\n",
    "                                finding = prefix + finding\n",
    "                            results.append((finding, lr_value))\n",
    "                sibling = sibling.find_next_sibling()\n",
    "    else:\n",
    "        # Fallback: process all tables in the section if no <h3> headings exist.\n",
    "        tables = lr_section.find_all('table', class_='lrtable')\n",
    "        for table in tables:\n",
    "            rows = table.find_all(\"tr\")\n",
    "            data_rows = [row for row in rows if row.find_all(\"td\")]\n",
    "            if data_rows:\n",
    "                for row in data_rows:\n",
    "                    cols = row.find_all(\"td\")\n",
    "                    if len(cols) >= 2:\n",
    "                        finding = cols[0].get_text(strip=True)\n",
    "                        lr_value = cols[1].get_text(strip=True)\n",
    "                        link = cols[1].find(\"a\")\n",
    "                        if link:\n",
    "                            lr_value = link.get_text(strip=True) or lr_value\n",
    "                        if not lr_value:\n",
    "                            lr_value = \"Not reported\"\n",
    "                        results.append((finding, lr_value))\n",
    "            else:\n",
    "                all_tds = table.find_all(\"td\")\n",
    "                for i in range(0, len(all_tds), 2):\n",
    "                    finding = all_tds[i].get_text(strip=True)\n",
    "                    if i+1 < len(all_tds):\n",
    "                        lr_value = all_tds[i+1].get_text(strip=True)\n",
    "                    else:\n",
    "                        lr_value = \"Not reported\"\n",
    "                    a_tag = all_tds[i+1].find(\"a\") if i+1 < len(all_tds) else None\n",
    "                    if a_tag:\n",
    "                        lr_value = a_tag.get_text(strip=True) or lr_value\n",
    "                    if not lr_value:\n",
    "                        lr_value = \"Not reported\"\n",
    "                    results.append((finding, lr_value))\n",
    "                    \n",
    "    return results\n",
    "\n",
    "\"\"\"\n",
    "def extract_likelihood_ratios(page_content):\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    results = []\n",
    "\n",
    "    # Locate the section containing likelihood ratio tables\n",
    "    lr_section = soup.find('article', class_='lr_cards_details')\n",
    "    if not lr_section:\n",
    "        return results  # Return empty if no section found\n",
    "\n",
    "    # Find all tables inside the LR card\n",
    "    tables = lr_section.find_all('table', class_='lrtable')\n",
    "\n",
    "    for table in tables:\n",
    "        # Grab all <tr> elements\n",
    "        all_rows = table.find_all('tr')\n",
    "\n",
    "        # Filter out any row that only has <th> (i.e., a header row)\n",
    "        data_rows = []\n",
    "        for row in all_rows:\n",
    "            # If there's at least one <td> in this row, treat it as a data row\n",
    "            if row.find_all('td'):\n",
    "                data_rows.append(row)\n",
    "\n",
    "        # If we have real data rows, parse them\n",
    "        if data_rows:\n",
    "            for row in data_rows:\n",
    "                cols = row.find_all('td')\n",
    "                # If the row has exactly 2 <td>, treat them as (finding, LR)\n",
    "                if len(cols) == 2:\n",
    "                    finding = cols[0].get_text(strip=True)\n",
    "                    lr_value = cols[1].get_text(strip=True)\n",
    "                    # If there's an <a> inside the LR cell, grab its text\n",
    "                    link = cols[1].find('a')\n",
    "                    if link:\n",
    "                        lr_value = link.get_text(strip=True) or lr_value\n",
    "                    if not lr_value:\n",
    "                        lr_value = \"Not reported\"\n",
    "\n",
    "                    results.append((finding, lr_value))\n",
    "\n",
    "        else:\n",
    "            # Fallback: if there are no valid data rows, we process all <td> in pairs\n",
    "            cols = table.find_all('td')\n",
    "            for i in range(0, len(cols) - 1, 2):\n",
    "                finding = cols[i].get_text(strip=True)\n",
    "                lr_value_element = cols[i + 1]\n",
    "\n",
    "                # Extract the likelihood ratio, handling nested <a> and <br/>\n",
    "                link = lr_value_element.find('a')\n",
    "                if link:\n",
    "                    lr_value = link.get_text(strip=True)\n",
    "                else:\n",
    "                    lr_value = lr_value_element.get_text(strip=True)\n",
    "\n",
    "                if not lr_value:\n",
    "                    lr_value = \"Not reported\"\n",
    "\n",
    "                results.append((finding, lr_value))\n",
    "\n",
    "    return results\n",
    "\"\"\"\n",
    "    \n",
    "def fetch_webpages(specialty_links):\n",
    "    \"\"\"\n",
    "    Iterates through all the extracted links, fetches the webpage content, \n",
    "    and extracts likelihood ratio findings.\n",
    "    \"\"\"\n",
    "    findings_by_display_name = {}\n",
    "\n",
    "    for item in specialty_links:\n",
    "        print(f\"Fetching pages for Specialty: {item['specialty']}\")\n",
    "\n",
    "        for link in item['links']:\n",
    "            display_name = link['display_name']\n",
    "            url = link['url']\n",
    "\n",
    "            try:\n",
    "                print(f\"  - Fetching: {display_name} ({url})\")\n",
    "                response = requests.get(url)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    print(f\"    Success: {display_name} page fetched.\")\n",
    "                    \n",
    "                    # Extract likelihood ratio findings\n",
    "                    findings = extract_likelihood_ratios(response.text)\n",
    "                    \n",
    "                    # Store the extracted data\n",
    "                    findings_by_display_name[display_name] = findings\n",
    "\n",
    "                else:\n",
    "                    print(f\"    Failed to fetch {display_name} - Status Code: {response.status_code}\")\n",
    "\n",
    "                time.sleep(1)  # Optional: Add a delay to avoid overwhelming the server\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"    Error fetching {display_name}: {e}\")\n",
    "\n",
    "        print(\"\\n\")  # Add space between specialties for readability\n",
    "\n",
    "    return findings_by_display_name\n",
    "\n",
    "def save_to_excel(findings_data, filename=\"nnt_lrs.xlsx\", blank_values=False):\n",
    "    \"\"\"\n",
    "    Saves likelihood ratios to an Excel file with each display_name as a separate sheet.\n",
    "    If blank_values is True, the Likelihood Ratio column is left blank.\n",
    "    The first row contains the full display_name, and column headers start from the second row.\n",
    "    \"\"\"\n",
    "    with pd.ExcelWriter(filename, engine=\"openpyxl\") as writer:\n",
    "        for display_name, findings in findings_data.items():\n",
    "            if findings:\n",
    "                # Prepare DataFrame\n",
    "                df = pd.DataFrame(findings, columns=[\"Finding\", \"Likelihood Ratio\"])\n",
    "\n",
    "                if blank_values:\n",
    "                    df[\"Likelihood Ratio\"] = \"\"  # Clear likelihood ratio values\n",
    "\n",
    "                # Insert full display_name as the first row\n",
    "                full_name_row = pd.DataFrame({df.columns[0]: [display_name], df.columns[1]: [\"\"]})\n",
    "                df = pd.concat([full_name_row, df], ignore_index=True)\n",
    "\n",
    "                # Save to Excel with sheet name as the **last** 31 characters\n",
    "                sheet_name = display_name[-31:]\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)  # No default header\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping {display_name} (No data found).\")\n",
    "\n",
    "    print(f\"\\nLikelihood ratios saved to {filename}\")\n",
    "\n",
    "# Fetch specialties and links\n",
    "specialty_links = get_specialty_links()\n",
    "findings_data = fetch_webpages(specialty_links)\n",
    "\n",
    "# Save normal file\n",
    "save_to_excel(findings_data, \"nnt_lrs.xlsx\", blank_values=False)\n",
    "\n",
    "# Save version with blank likelihood ratios\n",
    "save_to_excel(findings_data, \"nnt_lrs_sans_number.xlsx\", blank_values=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert LR's to numerical format\n",
    "\n",
    "This block takes in the excel spreadsheet with raw data from theNNT.com (\"nnt_lrs.xlsx\") and creates a new spreadsheet (\"nnt_lrs_processed.xlsx\") with a third column that contains a numerical version of the second column (raw data from theNNT) to be used as the LR_llm.\n",
    "\n",
    "\n",
    "It removes any 'x's from the input, then determines whether the cell reports \n",
    "\n",
    "1. point estimate only (in which case use the point estimate)\n",
    "2. point estimate + range (in which case take the point estimate), or \n",
    "3. range only (in which case, calculate the geometric mean)\n",
    "\n",
    "It also counts the number of conditions (last 31 letters due to excel limitation) and LRs\n",
    "\n",
    "\n",
    "[ ] TODO: \n",
    "- some of the BNP thresholds just have a number rather than a specification of the full \"BNP > 100\"; need to use header e.g. https://thennt.com/lr/dyspnea-due-to-heart-failure-without-chronic-respiratory-disease/ and https://thennt.com/lr/dyspnea-due-to-heart-failure-without-chronic-respiratory-disease/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_lr(lr_str):\n",
    "    \"\"\"\n",
    "    Given a string from the 'Likelihood Ratio' cell, this function:\n",
    "      - Removes any 'x' characters from the input.\n",
    "      - If the string contains a parenthesized range (i.e. a point estimate plus a range),\n",
    "        it returns the point estimate.\n",
    "      - If the entire string is a range (e.g. \"0.92-1.1\", \"3.3 to 4.8\", \"4.8–7.6\"),\n",
    "        it computes and returns the geometric mean.\n",
    "      - Otherwise, it returns a float based on the first number found.\n",
    "      - If the value is missing or cannot be parsed, returns NaN.\n",
    "    \"\"\"\n",
    "    # Remove all 'x' characters and trim whitespace\n",
    "    lr_str = lr_str.replace(\"x\", \"\").strip()\n",
    "    if lr_str == \"\":\n",
    "        return np.nan\n",
    "\n",
    "    # If parentheses exist, assume format \"point_estimate (range)\" and use the point estimate.\n",
    "    if \"(\" in lr_str:\n",
    "        point_part = lr_str.split(\"(\")[0].strip()\n",
    "        try:\n",
    "            return float(point_part)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Check for a range-only pattern.\n",
    "    # This regex looks for two numbers separated by \"to\", \"-\" or \"–\" with optional whitespace.\n",
    "    range_only_match = re.match(r'^\\s*([0-9]*\\.?[0-9]+)\\s*(to|[-–])\\s*([0-9]*\\.?[0-9]+)\\s*$', lr_str)\n",
    "    if range_only_match:\n",
    "        try:\n",
    "            low = float(range_only_match.group(1))\n",
    "            high = float(range_only_match.group(3))\n",
    "            return math.sqrt(low * high)\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    # Fallback: if no range-only pattern is found, extract the first number and return it.\n",
    "    numbers = re.findall(r'([0-9]*\\.?[0-9]+)', lr_str)\n",
    "    if numbers:\n",
    "        try:\n",
    "            return float(numbers[0])\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "\n",
    "    return np.nan\n",
    "\n",
    "# Load the original Excel file (each sheet has no header and the first row is the display name row)\n",
    "input_filename = \"nnt_lrs.xlsx\"\n",
    "output_filename = \"nnt_lrs_processed.xlsx\"\n",
    "\n",
    "# Read all sheets from the Excel file into a dictionary of DataFrames.\n",
    "excel_sheets = pd.read_excel(input_filename, sheet_name=None, header=None)\n",
    "\n",
    "total_lr_count = 0\n",
    "sheet_counts = {}\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine=\"openpyxl\") as writer:\n",
    "    for sheet_name, df in excel_sheets.items():\n",
    "        numerical_lr = []\n",
    "        # Process each row in the sheet.\n",
    "        for idx, row in df.iterrows():\n",
    "            # For the header row (assumed to be the first row: condition label), add an empty string.\n",
    "            if idx == 0:\n",
    "                numerical_lr.append(\"\")\n",
    "            else:\n",
    "                cell_value = row[1]  # The original \"Likelihood Ratio\" is in the second column (index 1)\n",
    "                if pd.isna(cell_value) or str(cell_value).strip() == \"\":\n",
    "                    numerical_lr.append(\"\")\n",
    "                else:\n",
    "                    numerical_lr.append(parse_lr(str(cell_value)))\n",
    "        \n",
    "        # Insert the new column immediately after the \"Likelihood Ratio\" column.\n",
    "        # This makes the new column the third column.\n",
    "        df.insert(2, \"Numerical LR\", numerical_lr)\n",
    "        \n",
    "        # Remove rows (except the header) where the new \"Numerical LR\" is empty or NaN.\n",
    "        header = df.iloc[[0]]  # Keep the header row (the condition label)\n",
    "        data = df.iloc[1:]\n",
    "        data = data[data[\"Numerical LR\"].apply(lambda x: not (x == \"\" or pd.isna(x)))]\n",
    "        filtered_df = pd.concat([header, data], ignore_index=True)\n",
    "        \n",
    "        # Insert a new row (after the condition label row) with the column labels.\n",
    "        # The final sheet will have:\n",
    "        #   Row 0: Condition label (from the original sheet)\n",
    "        #   Row 1: Column labels: 'finding', 'lr_raw', and 'lr_num'\n",
    "        #   Row 2+: Data rows\n",
    "        col_labels = pd.DataFrame([[\"finding\", \"lr_raw\", \"lr_reported\"]], columns=filtered_df.columns)\n",
    "        final_df = pd.concat([filtered_df.iloc[[0]], col_labels, filtered_df.iloc[1:]], ignore_index=True)\n",
    "        \n",
    "        # Count the number of LR values for this sheet (exclude the two header rows).\n",
    "        lr_count = len(final_df) - 2\n",
    "        sheet_counts[sheet_name] = lr_count\n",
    "        total_lr_count += lr_count\n",
    "        \n",
    "        # Write the modified DataFrame to the new Excel file.\n",
    "        # The output maintains the original format: no index and no additional header row.\n",
    "        final_df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "# Display counts.\n",
    "num_sheets = len(excel_sheets)\n",
    "print(f\"Processed {num_sheets} condition(s) (sheets).\")\n",
    "for sheet, count in sheet_counts.items():\n",
    "    print(f\"Sheet '{sheet}' has {count} LR value(s).\")\n",
    "print(f\"Total LR values processed across all sheets: {total_lr_count}.\")\n",
    "\n",
    "print(f\"Processed Excel file saved as '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate LRs\n",
    "\n",
    "NOTE: for the real run at this, we'll want to do some manual editing of the info columns - as there are some where it is a lab value that references the preceeding value (not currently automated to account for). \n",
    "--- particularly, BNP thresholds in the cardiac-cause sheets. \n",
    "\n",
    "\n",
    "This code block reads in the data from the nnt_lr_processed.xlsx excel file and calls a list of openAI models to have them give there best (single) estimate of the LR. Then, it rights a new spreadsheet nnt_lr_estimates that includes columns in each spreadsheet for each estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the response schema expecting a floating point number.\n",
    "class LRResponse(BaseModel):\n",
    "    value: float\n",
    "\n",
    "def estimate_lr(diagnosis: str, info_val: str, client, model: str) -> float:\n",
    "    \"\"\"\n",
    "    Calls the LLM with a prompt containing the diagnosis and a finding.\n",
    "    Returns the estimated likelihood ratio as a floating point number.\n",
    "    \"\"\"\n",
    "    lr_prompt = \"\"\"You are an expert in medical diagnosis who is giving assessments of how important a piece of information is when determining whether a patient has a particularly condition. Your task is to estimate the likelihood ratio of a finding for a disease. Recall that the likelihood ratio represents how much the ratio between the odds of disease given a result for a lab value, whether a physical exam finding is present, or whether a comorbidity is present over the odds of disease when you did not know the result.\n",
    "You will receive inputs in the following format; Target condition: <Condition, e.g. Patient Has: Cardiac chest pain>. Finding: <piece of information, e.g. ‘Patient does not have: radiation to the neck, arm, or jaw’>.\n",
    "So, for example. If the odds of a Condition Z being present was 1 (meaning 50% probability) before we knew anything, but then we got a result (Finding A) it became 2 (meaning 2:1 odds or 66% probability), then the likelihood ratio would be 2. \n",
    "Given a condition and a finding, you will provide your best estimate of the likelihood ratio as a floating point number. Return your answer in valid JSON with the following schema: { 'value': <floating point number greater than 0> }.\\n\\n\n",
    "\n",
    "Remember, stronger evidence in favor of a condition has a value farther above 1. Strong evidence against a diagnosis has a value farther below 1 (closer to 0). A likelihood ratio of 10 is equally strong evidence for a condition as a likelihood ratio of 0.1 is against it. Likelihood ratios near 1 represent weak evidence for or against. \n",
    "And if the \"patient does not have: \" some feature that is almost always present, that is strong evidence against.\n",
    "(pay attention for double negatives- Patient has: no tobacco and Patient does not have: tobacco are identical)\n",
    "\n",
    "Here is how I would like you to approach the problem:\n",
    "First, consider the condition you are predicting (Condition: ___). Is the condition a medical diagnosis? If so, what kind of findings are usually present in someone who has that condition. Does the condition specify a certain type of patient? If so, how does that change things? \n",
    "Then, consider the finding. If a finding is much more common among patients who have the condition of interest than among patients who do not have the condition of interest, then the likelihood ratio should be high. This might be because the finding is a consequence of the disease, indicates that an enabling condition is present, indicates that a frequently comorbid condition is present, or is related to the pathology of the condition. In general, likelihood ratios over about 20 are pathognomonic, above 5 or so is extremely strong evidence in favor, above 2.5 or so is strong evidence, above 1.4 is so-so evidence, and 1-1.4 is pretty weak evidence. Conversely, if the finding is more common in people who do NOT have the condition, then the likelihood ratio should be below 1. Similarly, a likelihood ratio below 0.05 would exclude the condition in most situations, below 0.2 would be extremely strong evidence against, below 0.4 would be strong evidence against, below 0.71 is so-so, and between 0.71 and 1 is pretty weak evidence against (meaning, it just doesn’t change the odds of the condition much). \n",
    "\n",
    "Here are some hypothetical examples to consider: \n",
    "    Prompt = Target condition: Cardiac Chest Pain. Finding: Patient has: Pain not worse with exertion (requires they clarify exercise 1hr after meal).\n",
    "    You would reason that because cardiac chest pain is usually worse with exertion because exertion worsens cardiac demand for oxygen, and thus worsens ischemia.\n",
    "    Response = {\n",
    "        ‘value’: 0.4\n",
    "    }\n",
    "\n",
    "    Prompt =  Target condition: Cardiac Chest Pain. Finding: Patient does not have: tobacco.\n",
    "    You would reason that because being someone who smokes increases your risk of coronary artery disease, and thus being a never smoker means you’re at less risk… but many people who have heart attacks still smoke, so it’s only a weak predictor. \n",
    "    Response = {\n",
    "        ‘value’: 0.75\n",
    "    }\n",
    "\n",
    "    Prompt = Target condition: Cardaic Chest Pain. Finding = Patient has: enjoys playing chess.\n",
    "    You would reason that because enjoying chest has no relationship to having a heart attack.\n",
    "    Response = {\n",
    "        ‘value’: 1\n",
    "    }\n",
    "\n",
    "    Prompt = Target condition: Cardiac Chest Pain. Finding = Patient has: pain located behind the sternum\n",
    "    You would reason that because cardiac chest pain is often experienced behind the sternum (thus, more likely), but so are many other causes of chest pain - like GERD.\n",
    "    Response = {\n",
    "        ‘value’: 1.2\n",
    "    }\n",
    "\n",
    "    Prompt = Condition: Cardiac Chest Pain. Finding = patient has: pain worse with exertion.\n",
    "    You would reason that because the increased myocardial oxygen consumption worsens the pain if oxygen delivery to the myocardium is the cause, as it is in heart attacks.\n",
    "    Response = {\n",
    "        ‘value’: 3.4\n",
    "    }\n",
    "\n",
    "    OK: here’s the prompt.. \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": lr_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Condition: {diagnosis}\\nFinding: {info_val}\"}\n",
    "    ]\n",
    "\n",
    "    # Check if the model starts with \"o3-mini\" \"o3\" \"o4-mini, \"o4\", etc.\n",
    "    kwargs = {}\n",
    "    if model.startswith(\"o\"):\n",
    "        kwargs[\"reasoning_effort\"] = \"medium\"  # low, medium, high depending on need\n",
    "\n",
    "    # Call the LLM using the provided model name.\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        response_format=LRResponse,\n",
    "        **kwargs  # pass the conditional keyword argument\n",
    "    )\n",
    "\n",
    "    # Call the LLM using the provided model name.\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=model,  # use the current model from the list\n",
    "        messages=messages,\n",
    "        response_format=LRResponse,\n",
    "    )\n",
    "    \n",
    "    # Extract and return the floating point estimate.\n",
    "    lr_response = completion.choices[0].message.parsed\n",
    "    return lr_response.value\n",
    "\n",
    "\n",
    "# Initialize the OpenAI (or your chosen) client using your API key.\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "# List of model names to iterate over.\n",
    "model_names = ['gpt-4o-mini-2024-07-18', 'gpt-4.1-mini-2025-04-14', 'gpt-4.1-2025-04-14', 'o4-mini-2025-04-16'] #'gpt-4o-2024-08-06', 'o3-mini-2025-01-31', gpt-4.1-2025-04-14, o4-mini-2025-04-16, 'o3-2025-04-16']\n",
    "\n",
    "# Read the processed Excel file.\n",
    "# We use header=None so that row 0 (the diagnosis row) and row 1 (the column headers row) are preserved.\n",
    "input_filename = \"nnt_lrs_processed.xlsx\"\n",
    "#input_filename = \"new_\" \\\n",
    "#\"nnt_lrs_processed.xlsx\" # use this one for the substitute in\n",
    "sheets = pd.read_excel(input_filename, sheet_name=None, header=None)\n",
    "\n",
    "# Process each sheet\n",
    "for sheet_name, df in sheets.items():\n",
    "    # Set diagnosis from the first row (row index 0, first cell)\n",
    "    diagnosis = df.iloc[0, 0]\n",
    "\n",
    "    # For each model in the list, call the LLM and add a new column with the estimation.\n",
    "    for model in model_names:\n",
    "        new_col_header = \"lr_\" + model\n",
    "        new_col = []  # This list will hold one value per row\n",
    "        print(f\"Diagnosis: '{diagnosis}', Model: '{model}'\")\n",
    "\n",
    "        # Iterate over each row in the sheet.\n",
    "        # Row 0 is the diagnosis row; row 1 is the existing column labels.\n",
    "        for i in range(len(df)):\n",
    "            if i == 0:\n",
    "                new_col.append(\"\")  # Leave the diagnosis row unchanged.\n",
    "            elif i == 1:\n",
    "                new_col.append(new_col_header)  # Insert the new column header in row 1.\n",
    "            else:\n",
    "                # For data rows, use the \"finding\" from the first column (index 0)\n",
    "                info_val = df.iloc[i, 0]\n",
    "                try:\n",
    "                    estimated_lr = estimate_lr(diagnosis, info_val, client, model)\n",
    "                except Exception as e:\n",
    "                    estimated_lr = \"ERROR\"  \n",
    "                    print(f\"Error estimating LR for sheet '{sheet_name}', row {i}, model {model}: {e}\")\n",
    "                new_col.append(estimated_lr)\n",
    "        \n",
    "        # Insert the new column at the end of the dataframe.\n",
    "        df.insert(df.shape[1], new_col_header, new_col)\n",
    "    \n",
    "    # Update the sheet data in our dictionary.\n",
    "    sheets[sheet_name] = df\n",
    "\n",
    "# Write out the modified sheets to a new Excel file.\n",
    "output_filename = \"nnt_lrs_with_estimated.xlsx\"\n",
    "with pd.ExcelWriter(output_filename, engine=\"openpyxl\") as writer:\n",
    "    for sheet_name, df in sheets.items():\n",
    "        # Write without adding pandas default headers or indices.\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)\n",
    "\n",
    "print(f\"Processed Excel file saved as '{output_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4o-mini-2024-07-18\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4o-2024-08-06\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4.1-mini-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4.1-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | o3-mini-2025-01-31\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | o3-2025-04-16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 182\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         lr = \u001b[43mestimate_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiagnosis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    184\u001b[39m         lr = \u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mestimate_lr\u001b[39m\u001b[34m(diagnosis, finding, client, model)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cap \u001b[38;5;129;01min\u001b[39;00m cfg[\u001b[33m\"\u001b[39m\u001b[33mcaps\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m         completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprimer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiagnosis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLRResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfield\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcap\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedium\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mo3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(completion.choices[\u001b[32m0\u001b[39m].message.parsed.value)\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    157\u001b[39m         \u001b[38;5;66;03m# Only retry on the length‑limit failure signature\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:180\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    175\u001b[39m         response_format=response_format,\n\u001b[32m    176\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    177\u001b[39m         input_tools=tools,\n\u001b[32m    178\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/openai/_base_client.py:1256\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/openai/_base_client.py:979\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    977\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    985\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bayesian LR estimator – tolerant of o‑family hidden‑reasoning bloat\n",
    "2025‑07‑07\n",
    "\"\"\"\n",
    "import os, logging, pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# SYSTEM PROMPT – still enforces JSON-only output\n",
    "# --------------------------------------------------------------------------\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a Bayesian-diagnostic assistant.\\n\"\n",
    "    \"Think step-by-step internally, then output only JSON:\\n\"\n",
    "    '{\"value\": <float> (>0)}.'\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# INTERNAL (HIDDEN) REASONING CHECKLIST\n",
    "# --------------------------------------------------------------------------\n",
    "INTERNAL_CHECKLIST = (\n",
    "    \"Use this silent checklist:\\n\"\n",
    "    \" 1. Recall the typical presentation.\\n\"\n",
    "    \" 2. Consider if the findings is present or absent.\\n\"\n",
    "    \" 3. Compare frequency of the finding (or its absence) in positives vs negatives.\\n\"\n",
    "    \" 4. Use the LR guide as a rough reference.\\n\"\n",
    "    \" 5. Refine your estimate within the LR-guide categories (e.g. 0.3 vs 0.4).\\n\"\n",
    "    \" 6. Return JSON only.\"\n",
    ")\n",
    "\n",
    "LR_LADDER = (\n",
    "    \"LR guide ➜  >20 pathognomonic · 5-20 very strong for · \"\n",
    "    \"2-5 moderate for · 1.4-2 weak for · 0.71-1.4 neutral · \"\n",
    "    \"0.2-0.71 moderate against · 0.05-0.2 very strong against · <0.05 rule-out.\"\n",
    ")\n",
    "\n",
    "# ----------------  9-SHOT PRIMER  (for gpt-family)  -------------------------\n",
    "RICH_PRIMER = \"\"\"\n",
    "<analysis># bucket: pathognomonic | step 1: prevalence high?\n",
    "C: bacterial meningitis\n",
    "F: nuchal rigidity\n",
    "</analysis>\n",
    "<json>{\"value\": 20}</json>\n",
    "\n",
    "<analysis># bucket: very-strong-for\n",
    "C: pulmonary embolism\n",
    "F: Wells score >6\n",
    "</analysis>\n",
    "<json>{\"value\": 10}</json>\n",
    "\n",
    "<analysis># bucket: strong-for\n",
    "C: ectopic pregnancy\n",
    "F: β-hCG >6500 IU + no intra-uterine sac\n",
    "</analysis>\n",
    "<json>{\"value\": 5}</json>\n",
    "\n",
    "<analysis># bucket: weak-for\n",
    "C: cardiac chest pain\n",
    "F: pain behind sternum\n",
    "</analysis>\n",
    "<json>{\"value\": 1.5}</json>\n",
    "\n",
    "<analysis># bucket: moderate-against\n",
    "C: appendicitis\n",
    "F: guarding absent\n",
    "</analysis>\n",
    "<json>{\"value\": 0.5}</json>\n",
    "\n",
    "<analysis># bucket: very-strong-against\n",
    "C: DKA\n",
    "F: normal anion gap\n",
    "</analysis>\n",
    "<json>{\"value\": 0.1}</json>\n",
    "\n",
    "<analysis># bucket: neutral\n",
    "C: myocardial infarction\n",
    "F: enjoys playing chess\n",
    "</analysis>\n",
    "<json>{\"value\": 1}</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "# --------------  2-SHOT PRIMER  (for o-family)  ----------------------------\n",
    "MINIMAL_PRIMER = \"\"\"\n",
    "<analysis># pathognomonic\n",
    "C: bacterial meningitis\n",
    "F: nuchal rigidity\n",
    "</analysis>\n",
    "<json>{\"value\": 20}</json>\n",
    "\n",
    "<analysis># neutral\n",
    "C: myocardial infarction\n",
    "F: enjoys playing chess\n",
    "</analysis>\n",
    "<json>{\"value\": 1}</json>\n",
    "\"\"\".strip()\n",
    "\n",
    "# ------------ CAPABILITY MAP (single source‑of‑truth for all models) ---------\n",
    "\n",
    "MODEL_CAPABILITIES = {\n",
    "    # family  | length‑field              | temp? | hidden‑token caps to try\n",
    "    \"gpt-4o-mini-2024-07-18\"  : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    \"gpt-4o-2024-08-06\"       : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    \"gpt-4.1-mini-2025-04-14\" : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    \"gpt-4.1-2025-04-14\"      : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    # o‑family – we escalate 512 → 1024 → 2048 if needed\n",
    "    \"o3-mini-2025-01-31\": {\"field\": \"max_completion_tokens\", \"temp\": False,\n",
    "                        \"caps\": [2048]},   # single generous cap\n",
    "    \"o3-2025-04-16\":      {\"field\": \"max_completion_tokens\", \"temp\": False,\n",
    "                        \"caps\": [2048]},\n",
    "    \"o4-mini-2025-04-16\": {\"field\": \"max_completion_tokens\", \"temp\": False,\n",
    "                        \"caps\": [2048]},\n",
    "}\n",
    "\"\"\"\n",
    "# Abbreviated test run\n",
    "MODEL_CAPABILITIES = {\n",
    "    # family  | length‑field              | temp? | hidden‑token caps to try\n",
    "    \"gpt-4o-mini-2024-07-18\"  : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]},\n",
    "    \"gpt-4.1-mini-2025-04-14\" : {\"field\": \"max_tokens\",            \"temp\": True,  \"caps\": [64]}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# ----------------------------- RESPONSE SCHEMA ------------------------------\n",
    "class LRResponse(BaseModel):\n",
    "    value: float\n",
    "\n",
    "# --------------------------- HELPER FUNCTIONS -------------------------------\n",
    "def build_messages(dx: str, finding: str, for_o: bool) -> list[dict]:\n",
    "    primer = MINIMAL_PRIMER if for_o else RICH_PRIMER\n",
    "    return [\n",
    "        {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"system\",    \"content\": INTERNAL_CHECKLIST},   # ← NEW\n",
    "        {\"role\": \"system\",    \"content\": LR_LADDER},\n",
    "        {\"role\": \"assistant\", \"content\": primer},\n",
    "        {\"role\": \"user\",      \"content\": f\"Condition: {dx}\\nFinding: {finding}\"},\n",
    "    ]\n",
    "\n",
    "def estimate_lr(diagnosis: str, finding: str, client: OpenAI, model: str) -> float:\n",
    "    cfg          = MODEL_CAPABILITIES[model]\n",
    "    is_reasoning = model.startswith(\"o\")\n",
    "\n",
    "    primer      = MINIMAL_PRIMER if is_reasoning else RICH_PRIMER\n",
    "    temperature = 0.20 if is_reasoning else 0.10\n",
    "\n",
    "    for cap in cfg[\"caps\"]:\n",
    "        try:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                model=model,\n",
    "                messages=build_messages(primer, diagnosis, finding),\n",
    "                response_format=LRResponse,\n",
    "                **{cfg[\"field\"]: cap},\n",
    "                **({\"temperature\": temperature} if cfg[\"temp\"] else {}),\n",
    "                **({\"reasoning_effort\": \"medium\"} if model.startswith(\"o3\") else {}),\n",
    "            )\n",
    "            return float(completion.choices[0].message.parsed.value)\n",
    "        except Exception as e:\n",
    "            # Only retry on the length‑limit failure signature\n",
    "            if \"length limit was reached\" in str(e) and cap != cfg[\"caps\"][-1]:\n",
    "                logging.warning(f\"Retrying {model} with larger cap \"\n",
    "                                f\"({cap}→{cfg['caps'][cfg['caps'].index(cap)+1]})\")\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "# -------------------------------  MAIN PIPE  --------------------------------\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "models = list(MODEL_CAPABILITIES)\n",
    "input_file, output_file = \"nnt_lrs_processed.xlsx\", \"nnt_lrs_with_estimated.xlsx\"\n",
    "sheets = pd.read_excel(input_file, sheet_name=None, header=None)\n",
    "\n",
    "for sheet_name, df in sheets.items():\n",
    "    diagnosis = df.iloc[0, 0]\n",
    "    for model in models:\n",
    "        new_header, col = \"lr_\" + model, []\n",
    "        print(f\"→ {diagnosis[:60]} | {model}\")\n",
    "        for i in range(len(df)):\n",
    "            if i == 0:           col.append(\"\")\n",
    "            elif i == 1:         col.append(new_header)\n",
    "            else:\n",
    "                try:\n",
    "                    lr = estimate_lr(diagnosis, df.iloc[i, 0], client, model)\n",
    "                except Exception as e:\n",
    "                    lr = \"ERROR\"\n",
    "                    logging.warning(f\"Error on sheet '{sheet_name}', row {i}, \"\n",
    "                                    f\"model {model}: {e}\")\n",
    "                col.append(lr)\n",
    "        df.insert(df.shape[1], new_header, col)\n",
    "    sheets[sheet_name] = df\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "    for name, frame in sheets.items():\n",
    "        frame.to_excel(writer, sheet_name=name, index=False, header=False)\n",
    "\n",
    "print(f\"✅  All done – results saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newest Version (Aug 22nd)\n",
    "\n",
    "GPT-5 Capabilities added and revised prompting strategy mildly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-5\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-5-mini\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4o-mini-2024-07-18\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4o-2024-08-06\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4.1-mini-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | gpt-4.1-2025-04-14\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | o3-mini-2025-01-31\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | o3-2025-04-16\n",
      "→ Diagnostic Accuracy of Ultrasound for Confirmation of Endotr | o4-mini-2025-04-16\n",
      "→ Factors Predicting Difficult Endotracheal Intubation | gpt-5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 167\u001b[39m\n\u001b[32m    164\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDone – results saved to \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODELS\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mrun_batch\u001b[39m\u001b[34m(input_file, output_file, models)\u001b[39m\n\u001b[32m    149\u001b[39m         col.append(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)      \u001b[38;5;66;03m# keep blank rows blank\u001b[39;00m\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     lr = \u001b[43mestimate_lr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiagnosis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    153\u001b[39m     lr = \u001b[33m\"\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mestimate_lr\u001b[39m\u001b[34m(diagnosis, finding, model, client)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cfg[\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    118\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] = {\u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mlow\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m resp = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mmsgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLRResponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Structured Outputs → Pydantic\u001b[39;49;00m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(resp.output_parsed.value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/openai/resources/responses/responses.py:995\u001b[39m, in \u001b[36mResponses.parse\u001b[39m\u001b[34m(self, text_format, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    988\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_response: Response) -> ParsedResponse[TextFormatT]:\n\u001b[32m    989\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parse_response(\n\u001b[32m    990\u001b[39m         input_tools=tools,\n\u001b[32m    991\u001b[39m         text_format=text_format,\n\u001b[32m    992\u001b[39m         response=raw_response,\n\u001b[32m    993\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `Response` instance into a `ParsedResponse`\u001b[39;49;00m\n\u001b[32m   1033\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m   1034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedResponse\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTextFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/openai/_base_client.py:1256\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/openai/_base_client.py:979\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    977\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    978\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    985\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/llm_py311/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bayesian LR estimator — Responses API + GPT‑5–aware (no max_output_tokens)\n",
    "Updated: 2025‑08‑22\n",
    "\n",
    "Changes vs. your prior block:\n",
    "• Responses API only (no Chat fallback).\n",
    "• No max_output_tokens (output is schema‑constrained and tiny).\n",
    "• Reasoning models use reasoning.effort=\"medium\"; no temperature/top_p.\n",
    "• GPT‑5(+mini) use text.verbosity=\"low\" (applied only where supported).\n",
    "• Includes these models: gpt‑5, gpt‑5‑mini, gpt‑4o‑mini‑2024‑07‑18, gpt‑4o‑2024‑08‑06,\n",
    "  gpt‑4.1‑mini‑2025‑04‑14, gpt‑4.1‑2025‑04‑14, o3‑mini‑2025‑01‑31, o3‑2025‑04‑16, o4‑mini‑2025‑04‑16.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0) Configuration\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Model registry: whether it’s a reasoning model, whether it supports verbosity,\n",
    "# and whether temperature is allowed.\n",
    "\n",
    "MODEL_CAPABILITIES = {\n",
    "    # GPT‑5 series (reasoning; supports text.verbosity; no temperature)\n",
    "    \"gpt-5\"        : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "    \"gpt-5-mini\"   : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "    \"gpt-5-nano\"   : {\"reasoning\": True,  \"verbosity\": True,  \"allow_temp\": False},\n",
    "\n",
    "    # GPT‑4.1 family (non‑reasoning; temperature OK); include snapshots + aliases\n",
    "    \"gpt-4.1-2025-04-14\" : {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4.1-mini-2025-04-14\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4.1-nano-2025-04-14\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "\n",
    "    # GPT‑4o family (non‑reasoning; temperature OK); prefer latest snapshot or alias\n",
    "    \"gpt-4o-2024-11-20\"  : {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "    \"gpt-4o-mini-2024-07-18\": {\"reasoning\": False, \"verbosity\": False, \"allow_temp\": True},\n",
    "\n",
    "    # o‑series (reasoning; no temperature)\n",
    "    \"o3-2025-04-16\" : {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "    \"o3-mini-2025-01-31\": {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "    \"o4-mini-2025-04-16\": {\"reasoning\": True,  \"verbosity\": False, \"allow_temp\": False},\n",
    "}\n",
    "MODELS = list(MODEL_CAPABILITIES)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "INPUT_FILE  = \"nnt_lrs_processed.xlsx\"\n",
    "OUTPUT_FILE = \"nnt_lrs_with_estimated.xlsx\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Prompt (concise; no chain‑of‑thought)\n",
    "# -----------------------------------------------------------------------------\n",
    "SYSTEM_CORE = \"\"\"You are a Bayesian diagnostic assistant.\n",
    "Estimate a numeric likelihood ratio (LR) for a finding with respect to a diagnosis.\n",
    "Return only a JSON object matching the schema: {\"value\": <float>}, where value > 0.\n",
    "\"\"\"\n",
    "\n",
    "DEFINITION = \"\"\"Definition:\n",
    "LR = P(finding | diagnosis) / P(finding | not-diagnosis)\n",
    "\"\"\"\n",
    "\n",
    "BANDS = \"\"\"Reference bands (orientation):\n",
    ">10 strong for; 5-10 moderate for; 2–5 weak for; \n",
    "0.5–2 negligible; \n",
    "0.2-0.5 weak against; 0.1-0.2 moderate against; ≤0.1 strong against\"\"\"\n",
    "\n",
    "# Few‑shot examples (assistant outputs are JSON only)\n",
    "FEW_SHOT_RICH = [\n",
    "    (\"bacterial meningitis\",   \"nuchal rigidity\",                                 20.0),\n",
    "    (\"ectopic pregnancy\",      \"β-hCG >6500 IU with no intra‑uterine sac\",        5.0),\n",
    "    (\"cardiac chest pain\",     \"retrosternal pain\",                                1.5),\n",
    "    (\"appendicitis\",           \"guarding absent\",                                  0.5),\n",
    "    (\"diabetic ketoacidosis\",  \"normal anion gap\",                                 0.1),\n",
    "    (\"myocardial infarction\",  \"enjoys playing chess\",                             1.0),\n",
    "]\n",
    "\n",
    "FEW_SHOT_MIN = [\n",
    "    (\"bacterial meningitis\",   \"nuchal rigidity\",                                 20.0),\n",
    "    (\"myocardial infarction\",  \"enjoys playing chess\",                             1.0),\n",
    "]\n",
    "\n",
    "def build_messages(diagnosis: str, finding: str, reasoning: bool) -> list[dict]:\n",
    "    msgs: list[dict] = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_CORE.strip()},\n",
    "        {\"role\": \"system\", \"content\": DEFINITION.strip()},\n",
    "        {\"role\": \"system\", \"content\": BANDS.strip()},\n",
    "    ]\n",
    "    examples = FEW_SHOT_MIN if reasoning else FEW_SHOT_RICH\n",
    "    for dx_ex, f_ex, v_ex in examples:\n",
    "        msgs.append({\"role\": \"user\",      \"content\": f\"Condition: {dx_ex}\\nFinding: {f_ex}\"})\n",
    "        msgs.append({\"role\": \"assistant\", \"content\": f'{{\"value\": {float(v_ex)}}}'})\n",
    "    msgs.append({\"role\": \"user\", \"content\": f\"Condition: {diagnosis}\\nFinding: {finding}\"})\n",
    "    return msgs\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2) Structured Outputs schema (Pydantic)\n",
    "# -----------------------------------------------------------------------------\n",
    "class LRResponse(BaseModel):\n",
    "    value: float\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3) One‑call estimator (Responses API)\n",
    "# -----------------------------------------------------------------------------\n",
    "def estimate_lr(diagnosis: str, finding: str, model: str, client: Optional[OpenAI] = None) -> float:\n",
    "    if client is None:\n",
    "        client = OpenAI()\n",
    "\n",
    "    cfg = MODEL_CAPABILITIES[model]\n",
    "    msgs = build_messages(diagnosis, finding, reasoning=cfg[\"reasoning\"])\n",
    "\n",
    "    kwargs = {}\n",
    "    if cfg[\"reasoning\"]:\n",
    "        kwargs[\"reasoning\"] = {\"effort\": \"medium\"}     # for GPT‑5 and o‑series\n",
    "        # no temperature/top_p\n",
    "    elif cfg[\"allow_temp\"]:\n",
    "        kwargs[\"temperature\"] = 0.2                    # allowed for 4o / 4.1\n",
    "\n",
    "    # Apply verbosity only where supported (GPT‑5 family)\n",
    "    if cfg[\"verbosity\"]:\n",
    "        kwargs[\"text\"] = {\"verbosity\": \"low\"}\n",
    "\n",
    "    resp = client.responses.parse(\n",
    "        model=model,\n",
    "        input=msgs,\n",
    "        text_format=LRResponse,    # Structured Outputs → Pydantic\n",
    "        **kwargs,\n",
    "    )\n",
    "    return float(resp.output_parsed.value)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4) Main pipeline: read workbook → append model columns → write output\n",
    "# -----------------------------------------------------------------------------\n",
    "def run_batch(input_file: str | Path, output_file: str | Path, models: list[str]) -> None:\n",
    "    sheets = pd.read_excel(input_file, sheet_name=None, header=None)\n",
    "\n",
    "    for sheet_name, df in sheets.items():\n",
    "        diagnosis = str(df.iloc[0, 0]).strip()\n",
    "        for model in models:\n",
    "            new_header = \"lr_\" + model\n",
    "            col = []\n",
    "            print(f\"→ {diagnosis[:60]} | {model}\")\n",
    "            for i in range(len(df)):\n",
    "                if i == 0:\n",
    "                    col.append(\"\")              # top-left cell (sheet label row)\n",
    "                elif i == 1:\n",
    "                    col.append(new_header)      # column header row\n",
    "                else:\n",
    "                    try:\n",
    "                        finding = str(df.iloc[i, 0]).strip()\n",
    "                        if not finding:\n",
    "                            col.append(\"\")      # keep blank rows blank\n",
    "                            continue\n",
    "                        lr = estimate_lr(diagnosis, finding, model, client)\n",
    "                    except Exception as e:\n",
    "                        lr = \"ERROR\"\n",
    "                        logging.warning(f\"Error on sheet '{sheet_name}', row {i}, model {model}: {e}\")\n",
    "                    col.append(lr)\n",
    "            # Insert as object dtype to accommodate strings like \"ERROR\"\n",
    "            df.insert(df.shape[1], new_header, pd.Series(col, dtype=\"object\"))\n",
    "        sheets[sheet_name] = df\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        for name, frame in sheets.items():\n",
    "            frame.to_excel(writer, sheet_name=name, index=False, header=False)\n",
    "\n",
    "    print(f\"Done – results saved to '{output_file}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_batch(INPUT_FILE, OUTPUT_FILE, MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block to generate the processing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 1.  Optional helper – keep the same “Feature Type” buckets   \n",
    "#  [ ] TODO: this does not seem like it works very well - just usual manual for now #\n",
    "# ------------------------------------------------------------------ #\n",
    "def classify_feature_type(text: str) -> str:\n",
    "    \"\"\"Heuristic that matches the legacy categories.\"\"\"\n",
    "    t = str(text).lower()\n",
    "\n",
    "    history = \"history:\" in t\n",
    "    sign    = (\"sign:\" in t) or (\"symptom\" in t)\n",
    "    score   = any(k in t for k in (\"score\", \"points\", \"rule\"))\n",
    "    test    = any(k in t for k in (\"test:\", \"lab\", \"troponin\", \"d‑dimer\"))\n",
    "    img     = any(k in t for k in (\n",
    "        \"ultrasound\", \"ct\", \"mri\", \"x‑ray\", \"radiograph\", \"imaging\",\n",
    "        \"echo\", \"angiogram\"))\n",
    "\n",
    "    if history and test: return \"History and Test\"\n",
    "    if history and img:  return \"History and imaging\"\n",
    "    if history:          return \"History_\"\n",
    "    if sign:             return \"Sign_symptom\"\n",
    "    if score:            return \"Score\"\n",
    "    if test:             return \"Test finding\"\n",
    "    if img:              return \"Imaging finding\"\n",
    "    return \"Diagnosis\"\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# 2.  Converter – writes **one** sheet (same name as in template)     #\n",
    "# ------------------------------------------------------------------ #\n",
    "def convert_lr_workbook(\n",
    "    *,                       # keyword‑only for clarity\n",
    "    input_file: str | Path,  # e.g. \"nnt_lrs_with_estimated.xlsx\"\n",
    "    template_file: str | Path,  # updated example workbook\n",
    "    output_file: str | Path = \"nnt_lrs_converted.xlsx\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    • Collapses all per‑condition tabs from `input_file` into a master frame.\n",
    "    • Adds the right‑most `condition` column (original tab name).\n",
    "    • Writes a single worksheet whose name matches the (only) sheet\n",
    "      found in `template_file`, with the header row stored as **row 1**\n",
    "      (no Excel column headers) to preserve downstream‑notebook compatibility.\n",
    "    \"\"\"\n",
    "    input_file    = Path(input_file)\n",
    "    template_file = Path(template_file)\n",
    "    output_file   = Path(output_file)\n",
    "\n",
    "    # -------- determine the sole sheet name from the template --------------\n",
    "    template_xls = pd.ExcelFile(template_file, engine=\"openpyxl\")\n",
    "    if len(template_xls.sheet_names) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Template has {len(template_xls.sheet_names)} sheets; \"\n",
    "            \"expected exactly one after pruning.\"\n",
    "        )\n",
    "    target_sheet = template_xls.sheet_names[0]  # e.g. \"Master\"\n",
    "\n",
    "    # ------------------- build master dataframe ----------------------------\n",
    "    in_xls = pd.ExcelFile(input_file, engine=\"openpyxl\")\n",
    "    frames = []\n",
    "\n",
    "    for tab in in_xls.sheet_names:\n",
    "        raw = pd.read_excel(in_xls, sheet_name=tab, header=None)\n",
    "\n",
    "        # Expect: row‑0 = diagnosis, row‑1 = column labels, row‑2+ = data\n",
    "        if raw.shape[0] < 3:\n",
    "            continue  # skip empty or malformed tabs\n",
    "\n",
    "        header = raw.iloc[1]\n",
    "        df = raw.iloc[2:].copy()\n",
    "        df.columns = header\n",
    "        df = df.loc[:, ~df.columns.isna()]  # drop unnamed columns\n",
    "\n",
    "        # back‑fill Feature Type if the sheet didn't have it\n",
    "        if \"Feature Type\" not in df.columns:\n",
    "            df[\"Feature Type\"] = df.iloc[:, 0].apply(classify_feature_type)\n",
    "\n",
    "        df[\"condition\"] = tab          # new right‑most column\n",
    "        frames.append(df)\n",
    "\n",
    "    if not frames:\n",
    "        raise ValueError(\"No valid data found in the input workbook.\")\n",
    "\n",
    "    master = pd.concat(frames, ignore_index=True)\n",
    "    # ensure 'condition' is the final column\n",
    "    master = master[[c for c in master.columns if c != \"condition\"] + [\"condition\"]]\n",
    "\n",
    "    # ------------------------ write single sheet ---------------------------\n",
    "    with pd.ExcelWriter(output_file, engine=\"openpyxl\") as writer:\n",
    "        # replicate legacy layout: header row lives **inside** the block\n",
    "        block = pd.concat(\n",
    "            [pd.DataFrame([master.columns], columns=master.columns), master],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "        block.to_excel(\n",
    "            writer,\n",
    "            sheet_name=target_sheet[:31],  # Excel 31‑char cap\n",
    "            index=False,\n",
    "            header=False,\n",
    "        )\n",
    "\n",
    "    print(f\"✅  Converted workbook written to: {output_file} \"\n",
    "          f\"(single sheet: '{target_sheet}')\")\n",
    "    \n",
    "\n",
    "# Run the conversion\n",
    "convert_lr_workbook(\n",
    "    input_file   = '/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/llm_estimate_lrs/nnt_lrs_with_estimated.xlsx',   # produced after LR loop\n",
    "    template_file= '/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/llm_estimate_lrs/Past Runs/example_NNT_LRs_PC_07.03.2025.xlsx',  # updated example\n",
    "    output_file  = '/Users/blocke/Box Sync/Residency Personal Files/Scholarly Work/Locke Research Projects/llm_estimate_lrs/new_NNT_LRs_07.08.2025.xlsx',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code block for trouble shooting the Scraper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# url = 'https://thennt.com/lr/dyspnea-due-to-heart-failure-without-chronic-respiratory-disease/'\n",
    "url = 'https://thennt.com/lr/diagnostic-accuracy-history-physical-examination-laboratory-testing-giant-cell-arteritis/'\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "print(soup)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
